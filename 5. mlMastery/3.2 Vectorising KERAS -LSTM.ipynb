{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer /Keras /1. preProcess with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU,LSTM, Embedding,Flatten, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='logs/keras-lstm-3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make results reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT_LENGTH = 695 # from previous notebook\n",
    "# vocab size needs to be taken care off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_pickle(\"./pickles/1_processed_pos_tag.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot two teen couple go church party drink dri...</td>\n",
       "      <td>358</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy bastard 's quick movie review damn y2k b...</td>\n",
       "      <td>144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie like make jaded movie viewer thankful in...</td>\n",
       "      <td>276</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>`` quest camelot `` warner bros first feature-...</td>\n",
       "      <td>313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis mentally unstable man undergo psychot...</td>\n",
       "      <td>402</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment\n",
       "0  plot two teen couple go church party drink dri...         358        0.0\n",
       "1  happy bastard 's quick movie review damn y2k b...         144        0.0\n",
       "2  movie like make jaded movie viewer thankful in...         276        0.0\n",
       "3  `` quest camelot `` warner bros first feature-...         313        0.0\n",
       "4  synopsis mentally unstable man undergo psychot...         402        0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorising usin Keras\n",
    "- Setting integer value to a string token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create the tokenizer\n",
    "tokenizer =Tokenizer(VOCAB_SIZE)\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(df_preprocessed[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34094"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oredered as per max. freq\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding Vocabulary for EMB. matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\": 1,\n",
       " 'film': 2,\n",
       " 'movie': 3,\n",
       " \"n't\": 4,\n",
       " 'one': 5,\n",
       " 'make': 6,\n",
       " 'like': 7,\n",
       " 'character': 8,\n",
       " 'get': 9,\n",
       " 'see': 10,\n",
       " 'go': 11,\n",
       " 'time': 12,\n",
       " 'good': 13,\n",
       " 'even': 14,\n",
       " 'scene': 15,\n",
       " 'well': 16,\n",
       " 'story': 17,\n",
       " 'would': 18,\n",
       " 'play': 19,\n",
       " 'take': 20,\n",
       " 'much': 21,\n",
       " 'also': 22,\n",
       " 'come': 23,\n",
       " 'give': 24,\n",
       " 'two': 25,\n",
       " 'way': 26,\n",
       " 'know': 27,\n",
       " 'first': 28,\n",
       " 'bad': 29,\n",
       " 'seem': 30,\n",
       " 'look': 31,\n",
       " 'end': 32,\n",
       " 'life': 33,\n",
       " 'year': 34,\n",
       " 'work': 35,\n",
       " 'thing': 36,\n",
       " 'could': 37,\n",
       " 'plot': 38,\n",
       " 'say': 39,\n",
       " 'find': 40,\n",
       " 'really': 41,\n",
       " 'little': 42,\n",
       " 'show': 43,\n",
       " 'people': 44,\n",
       " 'man': 45,\n",
       " 'think': 46,\n",
       " 'never': 47,\n",
       " 'star': 48,\n",
       " 'love': 49,\n",
       " 'director': 50,\n",
       " 'great': 51,\n",
       " 'best': 52,\n",
       " 'new': 53,\n",
       " 'try': 54,\n",
       " 'performance': 55,\n",
       " 'big': 56,\n",
       " 'many': 57,\n",
       " 'action': 58,\n",
       " 'actor': 59,\n",
       " 'u': 60,\n",
       " 'want': 61,\n",
       " 'watch': 62,\n",
       " 'role': 63,\n",
       " 'another': 64,\n",
       " 'use': 65,\n",
       " 'back': 66,\n",
       " 'become': 67,\n",
       " 'audience': 68,\n",
       " 'world': 69,\n",
       " 'something': 70,\n",
       " 'still': 71,\n",
       " 'act': 72,\n",
       " 'day': 73,\n",
       " 'turn': 74,\n",
       " \"'re\": 75,\n",
       " 'however': 76,\n",
       " 'old': 77,\n",
       " 'set': 78,\n",
       " 'every': 79,\n",
       " 'though': 80,\n",
       " 'part': 81,\n",
       " 'comedy': 82,\n",
       " 'real': 83,\n",
       " 'funny': 84,\n",
       " 'guy': 85,\n",
       " 'begin': 86,\n",
       " 'enough': 87,\n",
       " 'around': 88,\n",
       " 'tell': 89,\n",
       " 'leave': 90,\n",
       " 'cast': 91,\n",
       " 'long': 92,\n",
       " 'point': 93,\n",
       " 'last': 94,\n",
       " 'may': 95,\n",
       " 'young': 96,\n",
       " 'fact': 97,\n",
       " 'run': 98,\n",
       " 'actually': 99,\n",
       " 'right': 100,\n",
       " 'woman': 101,\n",
       " 'script': 102,\n",
       " 'minute': 103,\n",
       " 'feel': 104,\n",
       " 'name': 105,\n",
       " 'almost': 106,\n",
       " \"'ve\": 107,\n",
       " 'write': 108,\n",
       " 'effect': 109,\n",
       " 'nothing': 110,\n",
       " 'john': 111,\n",
       " 'lot': 112,\n",
       " 'although': 113,\n",
       " 'place': 114,\n",
       " 'screen': 115,\n",
       " 'ever': 116,\n",
       " 'since': 117,\n",
       " 'moment': 118,\n",
       " 'start': 119,\n",
       " 'line': 120,\n",
       " 'live': 121,\n",
       " 'original': 122,\n",
       " 'kill': 123,\n",
       " 'call': 124,\n",
       " 'friend': 125,\n",
       " 'help': 126,\n",
       " 'lead': 127,\n",
       " 'family': 128,\n",
       " 'high': 129,\n",
       " 'without': 130,\n",
       " 'three': 131,\n",
       " 'problem': 132,\n",
       " 'keep': 133,\n",
       " 'picture': 134,\n",
       " 'least': 135,\n",
       " 'happen': 136,\n",
       " 'girl': 137,\n",
       " 'quite': 138,\n",
       " 'sequence': 139,\n",
       " 'away': 140,\n",
       " 'course': 141,\n",
       " 'ca': 142,\n",
       " 'need': 143,\n",
       " 'far': 144,\n",
       " 'might': 145,\n",
       " \"'m\": 146,\n",
       " 'rather': 147,\n",
       " 'must': 148,\n",
       " 'anything': 149,\n",
       " 'hard': 150,\n",
       " 'lose': 151,\n",
       " 'fall': 152,\n",
       " 'include': 153,\n",
       " 'laugh': 154,\n",
       " 'yet': 155,\n",
       " 'job': 156,\n",
       " 'child': 157,\n",
       " 'put': 158,\n",
       " 'american': 159,\n",
       " 'follow': 160,\n",
       " 'wife': 161,\n",
       " 'open': 162,\n",
       " 'alien': 163,\n",
       " 'kind': 164,\n",
       " 'hour': 165,\n",
       " 'always': 166,\n",
       " 'fun': 167,\n",
       " 'reason': 168,\n",
       " 'bit': 169,\n",
       " 'sense': 170,\n",
       " 'home': 171,\n",
       " 'special': 172,\n",
       " \"'ll\": 173,\n",
       " 'feature': 174,\n",
       " 'attempt': 175,\n",
       " 'hollywood': 176,\n",
       " 'instead': 177,\n",
       " 'bring': 178,\n",
       " 'human': 179,\n",
       " 'war': 180,\n",
       " 'head': 181,\n",
       " 'night': 182,\n",
       " 'series': 183,\n",
       " 'black': 184,\n",
       " 'half': 185,\n",
       " 'probably': 186,\n",
       " 'let': 187,\n",
       " 'hand': 188,\n",
       " 'along': 189,\n",
       " 'kid': 190,\n",
       " 'move': 191,\n",
       " 'pretty': 192,\n",
       " 'mean': 193,\n",
       " 'men': 194,\n",
       " 'everything': 195,\n",
       " 'meet': 196,\n",
       " 'mind': 197,\n",
       " 'involve': 198,\n",
       " 'dialogue': 199,\n",
       " 'sure': 200,\n",
       " 'idea': 201,\n",
       " 'together': 202,\n",
       " 'create': 203,\n",
       " 'face': 204,\n",
       " 'money': 205,\n",
       " 'believe': 206,\n",
       " 'interest': 207,\n",
       " 'father': 208,\n",
       " 'fight': 209,\n",
       " 'whole': 210,\n",
       " 'death': 211,\n",
       " 'horror': 212,\n",
       " 'force': 213,\n",
       " 'shot': 214,\n",
       " 'direct': 215,\n",
       " 'save': 216,\n",
       " 'everyone': 217,\n",
       " 'do': 218,\n",
       " 'city': 219,\n",
       " 'appear': 220,\n",
       " 'music': 221,\n",
       " 'sex': 222,\n",
       " 'question': 223,\n",
       " 'talk': 224,\n",
       " 'second': 225,\n",
       " 'couple': 226,\n",
       " 'perhaps': 227,\n",
       " 'small': 228,\n",
       " 'release': 229,\n",
       " 'less': 230,\n",
       " 'case': 231,\n",
       " 'eye': 232,\n",
       " 'next': 233,\n",
       " 'brother': 234,\n",
       " 'especially': 235,\n",
       " '10': 236,\n",
       " 'mother': 237,\n",
       " 'relationship': 238,\n",
       " 'expect': 239,\n",
       " 'word': 240,\n",
       " 'completely': 241,\n",
       " '2': 242,\n",
       " 'rest': 243,\n",
       " 'whose': 244,\n",
       " 'late': 245,\n",
       " 'evil': 246,\n",
       " 'die': 247,\n",
       " 'different': 248,\n",
       " 'boy': 249,\n",
       " 'simply': 250,\n",
       " 'care': 251,\n",
       " 'writer': 252,\n",
       " 'book': 253,\n",
       " 'anyone': 254,\n",
       " 'dead': 255,\n",
       " 'change': 256,\n",
       " 'base': 257,\n",
       " 'school': 258,\n",
       " 'michael': 259,\n",
       " 'several': 260,\n",
       " 'joke': 261,\n",
       " 'top': 262,\n",
       " 'sound': 263,\n",
       " 'review': 264,\n",
       " 'true': 265,\n",
       " 'humor': 266,\n",
       " 'matter': 267,\n",
       " 'town': 268,\n",
       " 'suppose': 269,\n",
       " 'entire': 270,\n",
       " 'group': 271,\n",
       " 'lack': 272,\n",
       " 'house': 273,\n",
       " 'add': 274,\n",
       " 'comic': 275,\n",
       " 'soon': 276,\n",
       " 'main': 277,\n",
       " \"'d\": 278,\n",
       " 'someone': 279,\n",
       " 'hit': 280,\n",
       " 'tv': 281,\n",
       " 'game': 282,\n",
       " 'wrong': 283,\n",
       " 'interesting': 284,\n",
       " 'mr': 285,\n",
       " 'fan': 286,\n",
       " 'full': 287,\n",
       " 'david': 288,\n",
       " 'side': 289,\n",
       " 'else': 290,\n",
       " 'present': 291,\n",
       " 'either': 292,\n",
       " 'ask': 293,\n",
       " 'final': 294,\n",
       " 'unfortunately': 295,\n",
       " 'car': 296,\n",
       " 'murder': 297,\n",
       " 'enjoy': 298,\n",
       " 'viewer': 299,\n",
       " 'james': 300,\n",
       " 'wonder': 301,\n",
       " 'element': 302,\n",
       " 'voice': 303,\n",
       " 'stop': 304,\n",
       " 'deal': 305,\n",
       " 'close': 306,\n",
       " 'credit': 307,\n",
       " 'often': 308,\n",
       " 'return': 309,\n",
       " 'later': 310,\n",
       " 'style': 311,\n",
       " 'camera': 312,\n",
       " 'break': 313,\n",
       " 'provide': 314,\n",
       " 'behind': 315,\n",
       " 'surprise': 316,\n",
       " 'certainly': 317,\n",
       " 'support': 318,\n",
       " 'person': 319,\n",
       " 'power': 320,\n",
       " 'son': 321,\n",
       " 'hero': 322,\n",
       " 'stand': 323,\n",
       " 'scream': 324,\n",
       " 'result': 325,\n",
       " 'despite': 326,\n",
       " 'team': 327,\n",
       " 'nice': 328,\n",
       " 'learn': 329,\n",
       " 'title': 330,\n",
       " 'finally': 331,\n",
       " 'perfect': 332,\n",
       " 'early': 333,\n",
       " 'order': 334,\n",
       " 'video': 335,\n",
       " 'maybe': 336,\n",
       " 'killer': 337,\n",
       " 'note': 338,\n",
       " 'robert': 339,\n",
       " 'summer': 340,\n",
       " 'piece': 341,\n",
       " 'able': 342,\n",
       " 'past': 343,\n",
       " 'consider': 344,\n",
       " 'fine': 345,\n",
       " 'experience': 346,\n",
       " 'miss': 347,\n",
       " 'daughter': 348,\n",
       " 'classic': 349,\n",
       " 'strong': 350,\n",
       " 'theater': 351,\n",
       " 'situation': 352,\n",
       " 'production': 353,\n",
       " 'example': 354,\n",
       " 'view': 355,\n",
       " 'event': 356,\n",
       " 'short': 357,\n",
       " 'sort': 358,\n",
       " 'thriller': 359,\n",
       " 'white': 360,\n",
       " 'kevin': 361,\n",
       " 'deep': 362,\n",
       " 'joe': 363,\n",
       " 'realize': 364,\n",
       " 'talent': 365,\n",
       " 'dog': 366,\n",
       " 'hold': 367,\n",
       " 'body': 368,\n",
       " 'worth': 369,\n",
       " 'dark': 370,\n",
       " 'earth': 371,\n",
       " 'drama': 372,\n",
       " 'decide': 373,\n",
       " 'version': 374,\n",
       " 'level': 375,\n",
       " 'self': 376,\n",
       " 'light': 377,\n",
       " 'room': 378,\n",
       " 'heart': 379,\n",
       " 'nearly': 380,\n",
       " 'upon': 381,\n",
       " 'offer': 382,\n",
       " 'spend': 383,\n",
       " 'up': 384,\n",
       " 'plan': 385,\n",
       " 'violence': 386,\n",
       " 'major': 387,\n",
       " 'screenplay': 388,\n",
       " 'throughout': 389,\n",
       " 'cut': 390,\n",
       " 'cop': 391,\n",
       " 'hope': 392,\n",
       " 'manage': 393,\n",
       " 'beautiful': 394,\n",
       " 'ship': 395,\n",
       " 'direction': 396,\n",
       " 'figure': 397,\n",
       " 'exactly': 398,\n",
       " 'art': 399,\n",
       " 'computer': 400,\n",
       " 'jack': 401,\n",
       " 'dream': 402,\n",
       " 'obvious': 403,\n",
       " 'fail': 404,\n",
       " 'disney': 405,\n",
       " 'already': 406,\n",
       " 'others': 407,\n",
       " 'genre': 408,\n",
       " 'simple': 409,\n",
       " 'state': 410,\n",
       " 'entertain': 411,\n",
       " 'battle': 412,\n",
       " 'age': 413,\n",
       " 'five': 414,\n",
       " 'four': 415,\n",
       " 'guess': 416,\n",
       " 'space': 417,\n",
       " 'jackie': 418,\n",
       " 'king': 419,\n",
       " 'pay': 420,\n",
       " 'wait': 421,\n",
       " 'career': 422,\n",
       " 'sometimes': 423,\n",
       " 'flick': 424,\n",
       " 'form': 425,\n",
       " 'member': 426,\n",
       " 'novel': 427,\n",
       " 'sit': 428,\n",
       " 'number': 429,\n",
       " 'truly': 430,\n",
       " 'oscar': 431,\n",
       " 'hear': 432,\n",
       " 'pull': 433,\n",
       " 'waste': 434,\n",
       " 'tom': 435,\n",
       " 'husband': 436,\n",
       " '1': 437,\n",
       " 'read': 438,\n",
       " 'drive': 439,\n",
       " 'filmmaker': 440,\n",
       " 'chase': 441,\n",
       " 'lee': 442,\n",
       " 'allow': 443,\n",
       " 'type': 444,\n",
       " 'easy': 445,\n",
       " 'peter': 446,\n",
       " 'york': 447,\n",
       " 'touch': 448,\n",
       " 'god': 449,\n",
       " 'walk': 450,\n",
       " 'drug': 451,\n",
       " 'stay': 452,\n",
       " 'planet': 453,\n",
       " 'fiction': 454,\n",
       " 'the': 455,\n",
       " 'wo': 456,\n",
       " 'explain': 457,\n",
       " 'stupid': 458,\n",
       " 'parent': 459,\n",
       " 'sequel': 460,\n",
       " 'rise': 461,\n",
       " 'yes': 462,\n",
       " 'shoot': 463,\n",
       " 'saw': 464,\n",
       " 'fill': 465,\n",
       " 'quickly': 466,\n",
       " 'song': 467,\n",
       " 'tale': 468,\n",
       " 'twist': 469,\n",
       " 'romantic': 470,\n",
       " 'carry': 471,\n",
       " 'possible': 472,\n",
       " 'remember': 473,\n",
       " 'score': 474,\n",
       " 'remain': 475,\n",
       " 'chance': 476,\n",
       " 'bore': 477,\n",
       " 'extremely': 478,\n",
       " 'prove': 479,\n",
       " 'material': 480,\n",
       " 'detail': 481,\n",
       " 'mostly': 482,\n",
       " 'attention': 483,\n",
       " 'future': 484,\n",
       " 'gun': 485,\n",
       " 'single': 486,\n",
       " 'particularly': 487,\n",
       " 'de': 488,\n",
       " 'catch': 489,\n",
       " 'charm': 490,\n",
       " 'project': 491,\n",
       " 'paul': 492,\n",
       " 'quality': 493,\n",
       " 'police': 494,\n",
       " 'none': 495,\n",
       " 'co': 496,\n",
       " 'van': 497,\n",
       " 'hell': 498,\n",
       " 'eventually': 499,\n",
       " 'escape': 500,\n",
       " 'throw': 501,\n",
       " 'wild': 502,\n",
       " 'emotional': 503,\n",
       " 'science': 504,\n",
       " 'mention': 505,\n",
       " 'dr': 506,\n",
       " 'crime': 507,\n",
       " 'image': 508,\n",
       " 'grow': 509,\n",
       " 'out': 510,\n",
       " 'focus': 511,\n",
       " 'smith': 512,\n",
       " 'villain': 513,\n",
       " 'slow': 514,\n",
       " 'girlfriend': 515,\n",
       " 'mark': 516,\n",
       " 'alone': 517,\n",
       " 'win': 518,\n",
       " 'george': 519,\n",
       " 'rock': 520,\n",
       " 'obviously': 521,\n",
       " 'television': 522,\n",
       " 'send': 523,\n",
       " 'within': 524,\n",
       " 'in': 525,\n",
       " 'usually': 526,\n",
       " 'success': 527,\n",
       " 'among': 528,\n",
       " 'million': 529,\n",
       " 'premise': 530,\n",
       " 'middle': 531,\n",
       " 'large': 532,\n",
       " 'stuff': 533,\n",
       " 'poor': 534,\n",
       " '3': 535,\n",
       " 'complete': 536,\n",
       " 'speak': 537,\n",
       " 'across': 538,\n",
       " 'low': 539,\n",
       " 'except': 540,\n",
       " 'chris': 541,\n",
       " 'theme': 542,\n",
       " 'secret': 543,\n",
       " 'happy': 544,\n",
       " 'mission': 545,\n",
       " 'build': 546,\n",
       " 'water': 547,\n",
       " 'reality': 548,\n",
       " 'history': 549,\n",
       " 'steal': 550,\n",
       " 'whether': 551,\n",
       " 'subject': 552,\n",
       " 'wonderful': 553,\n",
       " 'local': 554,\n",
       " 'crew': 555,\n",
       " 'serious': 556,\n",
       " 'trouble': 557,\n",
       " 'oh': 558,\n",
       " 'important': 559,\n",
       " 'box': 560,\n",
       " 'motion': 561,\n",
       " 'effort': 562,\n",
       " 'near': 563,\n",
       " 'ryan': 564,\n",
       " 'understand': 565,\n",
       " 'agent': 566,\n",
       " 'develop': 567,\n",
       " 'impressive': 568,\n",
       " 'cool': 569,\n",
       " 'law': 570,\n",
       " 'america': 571,\n",
       " 'cover': 572,\n",
       " 'office': 573,\n",
       " 'entertainment': 574,\n",
       " 'aspect': 575,\n",
       " 'recent': 576,\n",
       " 'robin': 577,\n",
       " 'actress': 578,\n",
       " 'basically': 579,\n",
       " 'cause': 580,\n",
       " 'apparently': 581,\n",
       " 'studio': 582,\n",
       " 'easily': 583,\n",
       " 'williams': 584,\n",
       " 'screenwriter': 585,\n",
       " 'message': 586,\n",
       " 'mystery': 587,\n",
       " 'william': 588,\n",
       " 'somehow': 589,\n",
       " 'thought': 590,\n",
       " 'party': 591,\n",
       " 'bill': 592,\n",
       " 'pick': 593,\n",
       " 'discover': 594,\n",
       " 'lie': 595,\n",
       " 'wish': 596,\n",
       " 'fast': 597,\n",
       " 'street': 598,\n",
       " 'jones': 599,\n",
       " 'fear': 600,\n",
       " 'pace': 601,\n",
       " 'suspense': 602,\n",
       " 'doubt': 603,\n",
       " 'rich': 604,\n",
       " 'red': 605,\n",
       " 'batman': 606,\n",
       " 'smart': 607,\n",
       " 'straight': 608,\n",
       " 'attack': 609,\n",
       " 'difficult': 610,\n",
       " 'blood': 611,\n",
       " 'ago': 612,\n",
       " 'flaw': 613,\n",
       " 'certain': 614,\n",
       " 'romance': 615,\n",
       " 'producer': 616,\n",
       " 'country': 617,\n",
       " 'ben': 618,\n",
       " 'hilarious': 619,\n",
       " 'business': 620,\n",
       " 'presence': 621,\n",
       " 'popular': 622,\n",
       " 'effective': 623,\n",
       " 'fly': 624,\n",
       " 'due': 625,\n",
       " 'convince': 626,\n",
       " 'critic': 627,\n",
       " 'company': 628,\n",
       " 'approach': 629,\n",
       " 'clear': 630,\n",
       " 'answer': 631,\n",
       " 'annoy': 632,\n",
       " 'appeal': 633,\n",
       " 'scary': 634,\n",
       " 'dramatic': 635,\n",
       " 'general': 636,\n",
       " 'vampire': 637,\n",
       " 'forget': 638,\n",
       " 'produce': 639,\n",
       " 'draw': 640,\n",
       " 'class': 641,\n",
       " 'strange': 642,\n",
       " 'budget': 643,\n",
       " 'sexual': 644,\n",
       " 'fire': 645,\n",
       " 'surprisingly': 646,\n",
       " 'control': 647,\n",
       " 'anyway': 648,\n",
       " '4': 649,\n",
       " 'personal': 650,\n",
       " 'continue': 651,\n",
       " 'adult': 652,\n",
       " 'somewhat': 653,\n",
       " 'emotion': 654,\n",
       " 'harry': 655,\n",
       " 'third': 656,\n",
       " 'ability': 657,\n",
       " 'previous': 658,\n",
       " 'jim': 659,\n",
       " 'successful': 660,\n",
       " 'prison': 661,\n",
       " 'similar': 662,\n",
       " 'absolutely': 663,\n",
       " 'former': 664,\n",
       " 'deliver': 665,\n",
       " 'choice': 666,\n",
       " 'share': 667,\n",
       " 'rule': 668,\n",
       " 'sister': 669,\n",
       " 'student': 670,\n",
       " 'familiar': 671,\n",
       " 'intelligent': 672,\n",
       " 'excellent': 673,\n",
       " 'becomes': 674,\n",
       " 'date': 675,\n",
       " 'bob': 676,\n",
       " 'towards': 677,\n",
       " 'predictable': 678,\n",
       " 'powerful': 679,\n",
       " 'cinema': 680,\n",
       " 'giant': 681,\n",
       " 'marry': 682,\n",
       " 'teen': 683,\n",
       " 'beyond': 684,\n",
       " 'visual': 685,\n",
       " 'victim': 686,\n",
       " 'nature': 687,\n",
       " 'reveal': 688,\n",
       " 'serve': 689,\n",
       " 'r': 690,\n",
       " 'trailer': 691,\n",
       " 'b': 692,\n",
       " 'sam': 693,\n",
       " 'clever': 694,\n",
       " 'martin': 695,\n",
       " 'definitely': 696,\n",
       " 'and': 697,\n",
       " 'felt': 698,\n",
       " 'usual': 699,\n",
       " 'brilliant': 700,\n",
       " 'murphy': 701,\n",
       " 'musical': 702,\n",
       " 'stone': 703,\n",
       " 'blue': 704,\n",
       " 'free': 705,\n",
       " 'park': 706,\n",
       " 'bunch': 707,\n",
       " 'solid': 708,\n",
       " 'reach': 709,\n",
       " 'wear': 710,\n",
       " 'mess': 711,\n",
       " 'rating': 712,\n",
       " 'appearance': 713,\n",
       " 'sweet': 714,\n",
       " 'off': 715,\n",
       " 'dance': 716,\n",
       " 'potential': 717,\n",
       " 'seriously': 718,\n",
       " 'master': 719,\n",
       " 'huge': 720,\n",
       " 'promise': 721,\n",
       " 'track': 722,\n",
       " 'suspect': 723,\n",
       " 'destroy': 724,\n",
       " 'strike': 725,\n",
       " 'bond': 726,\n",
       " 'shock': 727,\n",
       " 'choose': 728,\n",
       " 'week': 729,\n",
       " 'land': 730,\n",
       " 'to': 731,\n",
       " 'favorite': 732,\n",
       " 'feeling': 733,\n",
       " 'search': 734,\n",
       " 'unlike': 735,\n",
       " 'amount': 736,\n",
       " 'l': 737,\n",
       " 'non': 738,\n",
       " 'perfectly': 739,\n",
       " 'ex': 740,\n",
       " 'travel': 741,\n",
       " 'decent': 742,\n",
       " 'likely': 743,\n",
       " 'adventure': 744,\n",
       " 'tone': 745,\n",
       " 'scott': 746,\n",
       " 'e': 747,\n",
       " 'join': 748,\n",
       " 'issue': 749,\n",
       " 'enjoyable': 750,\n",
       " 'immediately': 751,\n",
       " 'sign': 752,\n",
       " 'private': 753,\n",
       " 'monster': 754,\n",
       " 'treat': 755,\n",
       " 'frank': 756,\n",
       " 'cameron': 757,\n",
       " 'truman': 758,\n",
       " 'door': 759,\n",
       " 'ten': 760,\n",
       " 'train': 761,\n",
       " 'bruce': 762,\n",
       " 'air': 763,\n",
       " 'la': 764,\n",
       " 'dumb': 765,\n",
       " 'deserve': 766,\n",
       " 'handle': 767,\n",
       " 'cold': 768,\n",
       " 'overall': 769,\n",
       " 'cross': 770,\n",
       " 'inside': 771,\n",
       " 'capture': 772,\n",
       " 'impossible': 773,\n",
       " 'carter': 774,\n",
       " 'contain': 775,\n",
       " 'richard': 776,\n",
       " 'rat': 777,\n",
       " 'trek': 778,\n",
       " 'purpose': 779,\n",
       " 'merely': 780,\n",
       " 'stick': 781,\n",
       " 'race': 782,\n",
       " 'neither': 783,\n",
       " 'hop': 784,\n",
       " 'particular': 785,\n",
       " 'list': 786,\n",
       " 'ultimately': 787,\n",
       " 'depth': 788,\n",
       " 'step': 789,\n",
       " 'struggle': 790,\n",
       " 'modern': 791,\n",
       " 'truth': 792,\n",
       " 'silly': 793,\n",
       " 'club': 794,\n",
       " 'otherwise': 795,\n",
       " 'mar': 796,\n",
       " 'society': 797,\n",
       " 'sight': 798,\n",
       " 'tim': 799,\n",
       " 'term': 800,\n",
       " 'allen': 801,\n",
       " 'personality': 802,\n",
       " 'player': 803,\n",
       " 'government': 804,\n",
       " 'describe': 805,\n",
       " 'of': 806,\n",
       " 'political': 807,\n",
       " 'various': 808,\n",
       " 'talented': 809,\n",
       " 'opportunity': 810,\n",
       " 'west': 811,\n",
       " 'design': 812,\n",
       " 'development': 813,\n",
       " 'key': 814,\n",
       " 'succeed': 815,\n",
       " 'soundtrack': 816,\n",
       " 'female': 817,\n",
       " 'english': 818,\n",
       " 'army': 819,\n",
       " 'bear': 820,\n",
       " 'slightly': 821,\n",
       " 'cliche': 822,\n",
       " 'heavy': 823,\n",
       " 'steve': 824,\n",
       " '5': 825,\n",
       " 'pop': 826,\n",
       " 'buy': 827,\n",
       " 'month': 828,\n",
       " 'gag': 829,\n",
       " 'toy': 830,\n",
       " 'foot': 831,\n",
       " 'tension': 832,\n",
       " 'six': 833,\n",
       " 'beat': 834,\n",
       " 'compare': 835,\n",
       " 'lover': 836,\n",
       " 'grace': 837,\n",
       " 'hate': 838,\n",
       " 'disaster': 839,\n",
       " 'soldier': 840,\n",
       " 'drop': 841,\n",
       " 'eddie': 842,\n",
       " 'raise': 843,\n",
       " 'kiss': 844,\n",
       " 'pass': 845,\n",
       " 'over': 846,\n",
       " 'imagine': 847,\n",
       " 'chan': 848,\n",
       " 'today': 849,\n",
       " 'engage': 850,\n",
       " 'memorable': 851,\n",
       " 'max': 852,\n",
       " 'detective': 853,\n",
       " 'hill': 854,\n",
       " 'baby': 855,\n",
       " 'confuse': 856,\n",
       " 'require': 857,\n",
       " 'totally': 858,\n",
       " 'introduce': 859,\n",
       " 'background': 860,\n",
       " 'color': 861,\n",
       " 'roll': 862,\n",
       " 'front': 863,\n",
       " 'fi': 864,\n",
       " 'ape': 865,\n",
       " 'woody': 866,\n",
       " 'award': 867,\n",
       " 'wood': 868,\n",
       " 'animation': 869,\n",
       " 'leader': 870,\n",
       " 'machine': 871,\n",
       " 'rescue': 872,\n",
       " 'cheap': 873,\n",
       " 'thrill': 874,\n",
       " 'excite': 875,\n",
       " 'terrible': 876,\n",
       " 'doctor': 877,\n",
       " 'simon': 878,\n",
       " 'sci': 879,\n",
       " 'mary': 880,\n",
       " 'angel': 881,\n",
       " 'blow': 882,\n",
       " 'entirely': 883,\n",
       " 'jump': 884,\n",
       " 'steven': 885,\n",
       " 'actual': 886,\n",
       " 'creature': 887,\n",
       " 'british': 888,\n",
       " 'constantly': 889,\n",
       " 'quick': 890,\n",
       " 'standard': 891,\n",
       " 'double': 892,\n",
       " 'queen': 893,\n",
       " 'impact': 894,\n",
       " 'fantasy': 895,\n",
       " 'brief': 896,\n",
       " 'ridiculous': 897,\n",
       " 'cinematography': 898,\n",
       " 'typical': 899,\n",
       " 'nick': 900,\n",
       " 'tough': 901,\n",
       " 'officer': 902,\n",
       " 'atmosphere': 903,\n",
       " 'occur': 904,\n",
       " 'ground': 905,\n",
       " 'minor': 906,\n",
       " 'seven': 907,\n",
       " 'trip': 908,\n",
       " 'notice': 909,\n",
       " 'violent': 910,\n",
       " 'subtle': 911,\n",
       " 'beauty': 912,\n",
       " '8': 913,\n",
       " 'dollar': 914,\n",
       " 'fairly': 915,\n",
       " 'road': 916,\n",
       " 'island': 917,\n",
       " 'on': 918,\n",
       " 'ride': 919,\n",
       " 'admit': 920,\n",
       " 'suffer': 921,\n",
       " 'dull': 922,\n",
       " 'highly': 923,\n",
       " 'rush': 924,\n",
       " 'partner': 925,\n",
       " 'suddenly': 926,\n",
       " 'willis': 927,\n",
       " 'count': 928,\n",
       " 'survive': 929,\n",
       " 'boring': 930,\n",
       " 'fit': 931,\n",
       " 'grant': 932,\n",
       " 'store': 933,\n",
       " 'complex': 934,\n",
       " 'president': 935,\n",
       " 'college': 936,\n",
       " 'portray': 937,\n",
       " 'hank': 938,\n",
       " 'animal': 939,\n",
       " 'ii': 940,\n",
       " 'concept': 941,\n",
       " 'whatever': 942,\n",
       " 'bug': 943,\n",
       " 'indeed': 944,\n",
       " 'flat': 945,\n",
       " 'mike': 946,\n",
       " 'costume': 947,\n",
       " 'episode': 948,\n",
       " 'menace': 949,\n",
       " 'basic': 950,\n",
       " 'recently': 951,\n",
       " 'outside': 952,\n",
       " 'plenty': 953,\n",
       " 'godzilla': 954,\n",
       " 'brown': 955,\n",
       " 'titanic': 956,\n",
       " 'suit': 957,\n",
       " 'cute': 958,\n",
       " 'respect': 959,\n",
       " 'cinematic': 960,\n",
       " 'climax': 961,\n",
       " 'meanwhile': 962,\n",
       " 'hot': 963,\n",
       " 'awful': 964,\n",
       " 'common': 965,\n",
       " 'edit': 966,\n",
       " 'clearly': 967,\n",
       " 'x': 968,\n",
       " 'suggest': 969,\n",
       " 'crash': 970,\n",
       " 're': 971,\n",
       " 'century': 972,\n",
       " 'conclusion': 973,\n",
       " '90': 974,\n",
       " 'sean': 975,\n",
       " 'arrive': 976,\n",
       " 'believable': 977,\n",
       " 'arm': 978,\n",
       " 'lawyer': 979,\n",
       " 'recommend': 980,\n",
       " 'chemistry': 981,\n",
       " 'band': 982,\n",
       " 'possibly': 983,\n",
       " 'thin': 984,\n",
       " 'encounter': 985,\n",
       " 'jerry': 986,\n",
       " 'scientist': 987,\n",
       " 'realistic': 988,\n",
       " 'somewhere': 989,\n",
       " 'avoid': 990,\n",
       " 'carrey': 991,\n",
       " 'french': 992,\n",
       " 'twenty': 993,\n",
       " 'fashion': 994,\n",
       " 'buddy': 995,\n",
       " 'language': 996,\n",
       " 'period': 997,\n",
       " 'witch': 998,\n",
       " 'aside': 999,\n",
       " 'haunt': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(tokenizer.word_index)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data to abv to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 859 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq = tokenizer.texts_to_sequences(df_preprocessed[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"happy bastard 's quick movie review damn y2k bug 's got head start movie star jamie lee curtis another baldwin brother william time story regard crew tugboat come across deserted russian tech ship strangeness kick power back little know power within go gore bring action sequence virus still feel empty like movie go flash substance n't know crew really middle nowhere n't know origin take ship big pink flashy thing hit mir course n't know donald sutherland stumble around drunkenly throughout 's `` hey let 's chase people around robot `` acting average even like curtis 're likely get kick work halloween h20 sutherland waste baldwin well 's act like baldwin course real star stan winston 's robot design schnazzy cgi occasional good gore shot like pick someone 's brain robot body part really turn 's movie otherwise 's pretty much sunken ship movie\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_preprocessed['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2278"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(df_preprocessed['review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 544, 2388,    1,  890,    3,  264, 1095,  943,    1, 4785,  181,\n",
       "        119,    3,   48, 2666,  442, 2961,   64, 1400,  234,  588,   12,\n",
       "         17, 1847,  555,   23,  538, 1541, 3416,  395, 1022,  320,   66,\n",
       "         42,   27,  320,  524,   11, 1153,  178,   58,  139, 1560,   71,\n",
       "        104, 1783,    7,    3,   11, 1414, 1615,    4,   27,  555,   41,\n",
       "        531, 1289,    4,   27, 6713,   20,  395,   56, 4198, 2667,   36,\n",
       "        280, 9313,  141,    4,   27, 2886, 2814, 1542,   88,  389,    1,\n",
       "       1742,  187,    1,  441,   44,   88, 1802, 1561, 1063,   14,    7,\n",
       "       2961,   75,  743,    9, 1022,   35, 2094, 8609, 2814,  434, 1400,\n",
       "         16,    1,   72,    7, 1400,  141,   83,   48, 6714,    1, 1802,\n",
       "        812, 2294, 1923,   13, 1153,  214,    7,  593,  279,    1, 1023,\n",
       "       1802,  368,   81,   41,   74,    1,    3,  795,    1,  192,   21,\n",
       "        395,    3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "WE CAN SEE THE DIFFERENCE 10 , BCOZ OTHER VALUES ARE GRATER THEN 10000 SO ITS NOT CONSIDERED\n",
    "'''\n",
    "print(df_preprocessed['word_count'][1])\n",
    "print(len(np.array(seq[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Actual VOCAB_SIZE deined:  10000\n",
      " Actual tokens created:  34094\n"
     ]
    }
   ],
   "source": [
    "print(\" Actual VOCAB_SIZE deined: \",VOCAB_SIZE)\n",
    "print(\" Actual tokens created: \",len(tokenizer.word_index))# IF WE WANT TO CONSIDER EVERY TEXT IN VOCAB PASS THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Truncating Data\n",
    "* To feed it to N.N, inputs to have the same length\n",
    " - Either we ensure that all sequences in the entire data-set have the same length\n",
    " - Or Entier batch should be of same length\n",
    "* Going about choosing ampunt to pad\n",
    " - going with longest seq, would be just waste of memory for texr whose length is small\n",
    " - going with smalles seq , would be just ignoring other imp values \n",
    " - so we go optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 5595 6919  146   11  434  240]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the \n",
    "embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "'''\n",
    "print(np.array(seq[506]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and trucating here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_pad = sequence.pad_sequences(seq , maxlen=INPUT_TEXT_LENGTH,padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 695)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data_pad.shape\n",
    "# 2k review\n",
    "# and 695 fixed I/P shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "#imdb_data_pad[4]\n",
    "type(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x22c8b82c748>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Inverse Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"film extraordinarily horrendous 'm go waste word\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(seq[506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34094"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N.N Model\n",
    "len(tokenizer.word_index) # This are total word in dict\n",
    "#it depends if i want to use entier dict or only few occuring word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to test/ train / dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= imdb_data_pad #PADDED VERSION OF DATA\n",
    "y= df_preprocessed['sentiment'] # LABELS OF DATA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef export_test_train_data_to_h5(filename, X_train, X_test, y_train, y_test):\\n    #filename has to be xyzw.h5 with extension\\n    h5f = h5py.File(filename, \\'w\\')\\n    h5f.create_dataset(\\'X\\', data=X)           #np.ndarry\\n    h5f.create_dataset(\\'y\\', data=np.array(y)) #\\n    h5f.create_dataset(\\'X_train\\', data=X_train)           #np.ndarry\\n    h5f.create_dataset(\\'y_train\\', data=np.array(y_train)) #pd df\\n    h5f.create_dataset(\\'X_test\\', data=X_test)\\n    h5f.create_dataset(\\'y_test\\', data=np.array(y_test))\\n    # export weights of embedding_matrix incase of  \\n    h5f.close()\\n    print(\"file created\")\\n\\nfilename = \"2_2_keras.h5\"\\nexport_test_train_data_to_h5(filename,X_train, X_test, y_train, y_test)\\n\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def export_test_train_data_to_h5(filename, X_train, X_test, y_train, y_test):\n",
    "    #filename has to be xyzw.h5 with extension\n",
    "    h5f = h5py.File(filename, 'w')\n",
    "    h5f.create_dataset('X', data=X)           #np.ndarry\n",
    "    h5f.create_dataset('y', data=np.array(y)) #\n",
    "    h5f.create_dataset('X_train', data=X_train)           #np.ndarry\n",
    "    h5f.create_dataset('y_train', data=np.array(y_train)) #pd df\n",
    "    h5f.create_dataset('X_test', data=X_test)\n",
    "    h5f.create_dataset('y_test', data=np.array(y_test))\n",
    "    # export weights of embedding_matrix incase of  \n",
    "    h5f.close()\n",
    "    print(\"file created\")\n",
    "\n",
    "filename = \"2_2_keras.h5\"\n",
    "export_test_train_data_to_h5(filename,X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING A MODEL\n",
    "* http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 695, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 695, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Wall time: 360 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "                   #vocab_size\n",
    "model.add(Embedding(VOCAB_SIZE, 100, input_length=INPUT_TEXT_LENGTH))\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print out model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import os\n",
    "#install graph viz locally 1st\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'# install \n",
    "plot_model(model, to_file='3_2_LSTM_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 21s - loss: 0.6924 - acc: 0.5164\n",
      "Epoch 2/3\n",
      " - 20s - loss: 0.6301 - acc: 0.7664\n",
      "Epoch 3/3\n",
      " - 20s - loss: 0.3341 - acc: 0.8903\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#tensorboard = TensorBoard(log_dir='logs/3_2_LSTM_model')\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=2,callbacks=[tensorboard]) \n",
    "# use validation_split=0.08 only incase data is not been splitte earlier\n",
    "\n",
    "\n",
    "# TENSORBOARD\n",
    "'''\n",
    "# tensorboard --logdir logs/\n",
    "# http://localhost:6006/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.27%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Neural networks are stochastic, they can produce different results when the same model is fit on the same data.\n",
    "This is mainly because of the random initial weights and the shuffling of patterns during mini-batch gradient descent. \n",
    "This means that any one scoring of a model is unreliable and we should estimate model skill based on an average of multiple runs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6407241041010077, 0.7227272734497533]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refs\n",
    "#use tensorboard\n",
    "# 1. https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "# 2. Logloss is a very useful measure for evaluating the performance of learning algorithms on multi-class classification problems:\n",
    "# 3. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "'''\n",
    "#gives exact probab distribution\n",
    "y_pred = model.predict(x=X_test[0:500])\n",
    "y_pred = y_pred.T[0]\n",
    "OR\n",
    "y_pred_prob = model.predict_proba(x=X_test[0:500])\n",
    "y_pred_prob = y_pred_prob.T[0]\n",
    "'''\n",
    "# outputs by N.N directly to which class it belongs to\n",
    "y_pred_classes = model.predict_classes(x=X_test[0:660])\n",
    "cls_pred = y_pred_classes.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the actual true label\n",
    "cls_true = np.array(y_test[0:660])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing 2 classes\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   9,  11,  12,  16,  18,  21,  34,  36,  37,  38,  44,  50,\n",
       "        56,  58,  60,  63,  64,  72,  73,  83,  86,  89,  94,  95,  98,\n",
       "       103, 111, 113, 120, 122, 125, 128, 131, 133, 135, 136, 138, 139,\n",
       "       147, 149, 153, 159, 164, 165, 167, 169, 172, 179, 183, 186, 189,\n",
       "       192, 199, 201, 205, 206, 207, 208, 210, 215, 224, 230, 231, 233,\n",
       "       241, 244, 247, 250, 255, 258, 261, 262, 263, 265, 274, 278, 283,\n",
       "       291, 297, 298, 303, 305, 306, 312, 316, 317, 321, 326, 332, 338,\n",
       "       339, 340, 347, 348, 349, 352, 353, 359, 360, 366, 368, 374, 376,\n",
       "       377, 378, 380, 396, 408, 409, 412, 413, 419, 422, 426, 427, 429,\n",
       "       431, 432, 435, 436, 444, 447, 453, 455, 458, 463, 481, 484, 485,\n",
       "       488, 489, 490, 492, 496, 498], dtype=int64)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect  # all beow are idexes of mis-classified text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"arrive barrage hype blair witch project one big box office success year however like golden child although blair witch make lot money 's good donahue williams leonard play three student set make documentary blair witch myth film make camcorder footage record mean grainy footage woozy camera angle although event start normal get weird pretty quickly threesome argue journey go although interest premise blair witch project amount nothing missed opportunity big mistake film make let three mediocre actor chance improvise dialogue ad-libbed still manage sound like poor b-movie 'script eventually degenerate shout match f-word include lot sound like student breaks argument creepy event occur 's back shout swear get tiresome quickly want see three people get lose wood shout lot swear 'd go scout camp audience mean get entertainment factor 'm quite sure supernatural part film actually interest especially close attention pay story develop first twenty minute music budget work film depend natural psychological scare sometimes well deliver sometimes never really felt truly scared part film although small sense fear underlie throughout film however actor irritate scare lose eventually revert back heather say 'what f ck lot mike giggle like loony 's also niggling fact student filmmaker really stupid thing main problem fact even though hapless bunch could kill moment hopelessly lose heather still insist film film give half hearted reason want n't convince also student idea survive wood follow large river flow wood civilisation also part 'amateur camcorder footage obviously stag heather 's apology major one blair witch project end fail deliver suppose 've lose camping film may deliver chill use 99 pay audience n't lose wood apart final minute film mind boggingly unscary shout match get hideously dull spook scene short far horror movie could probably achieve amount fear scene provide although good idea 's execute well enough fun scary cinema experience 's worrying fact website http //www blairwitch com well film\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# direct checking from main df as X_test containt int representation of text\n",
    " df_preprocessed['review'][488]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual sentiment  0.0\n",
      "predicted sentiment  1\n"
     ]
    }
   ],
   "source": [
    "print(\"actual sentiment \",df_preprocessed['sentiment'][488])\n",
    "print(\"predicted sentiment \", cls_pred[488])\n",
    "# negative sentiment are zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "660"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
