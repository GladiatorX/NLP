{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refer /Keras /1. preProcess with keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GRU,LSTM, Embedding,Flatten, Dropout,CuDNNGRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "%matplotlib inline\n",
    "\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "tensorboard = TensorBoard(log_dir='logs/keras-lstm-3.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making it reproducable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_TEXT_LENGTH = 695 # from previous notebook\n",
    "# vocab size needs to be taken care off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessed = pd.read_pickle(\"./pickles/1_processed_pos_tag.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plot two teen couple go church party drink dri...</td>\n",
       "      <td>358</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>happy bastard 's quick movie review damn y2k b...</td>\n",
       "      <td>144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie like make jaded movie viewer thankful in...</td>\n",
       "      <td>276</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>`` quest camelot `` warner bros first feature-...</td>\n",
       "      <td>313</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>synopsis mentally unstable man undergo psychot...</td>\n",
       "      <td>402</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  word_count  sentiment\n",
       "0  plot two teen couple go church party drink dri...         358        0.0\n",
       "1  happy bastard 's quick movie review damn y2k b...         144        0.0\n",
       "2  movie like make jaded movie viewer thankful in...         276        0.0\n",
       "3  `` quest camelot `` warner bros first feature-...         313        0.0\n",
       "4  synopsis mentally unstable man undergo psychot...         402        0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorising usin Keras\n",
    "- Setting integer value to a string token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 394 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# create the tokenizer\n",
    "tokenizer =Tokenizer(VOCAB_SIZE)\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(df_preprocessed[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34094"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#oredered as per max. freq\n",
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# lookin abv result i feel numbers 1,2,3.. and 2char word like = ll,1char word like = x,y,z ' we can remove it\\n#provided we use cNN or ... maybe but not sure\\n# i'm not sure though its effect on LSTM network\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# lookin abv result i feel numbers 1,2,3.. and 2char word like = ll,1char word like = x,y,z ' we can remove it\n",
    "#provided we use cNN or ... maybe but not sure\n",
    "# i'm not sure though its effect on LSTM network\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding Vocabulary for EMB. matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\": 1,\n",
       " 'film': 2,\n",
       " 'movie': 3,\n",
       " \"n't\": 4,\n",
       " 'one': 5,\n",
       " 'make': 6,\n",
       " 'like': 7,\n",
       " 'character': 8,\n",
       " 'get': 9,\n",
       " 'see': 10,\n",
       " 'go': 11,\n",
       " 'time': 12,\n",
       " 'good': 13,\n",
       " 'even': 14,\n",
       " 'scene': 15,\n",
       " 'well': 16,\n",
       " 'story': 17,\n",
       " 'would': 18,\n",
       " 'play': 19,\n",
       " 'take': 20,\n",
       " 'much': 21,\n",
       " 'also': 22,\n",
       " 'come': 23,\n",
       " 'give': 24,\n",
       " 'two': 25,\n",
       " 'way': 26,\n",
       " 'know': 27,\n",
       " 'first': 28,\n",
       " 'bad': 29,\n",
       " 'seem': 30,\n",
       " 'look': 31,\n",
       " 'end': 32,\n",
       " 'life': 33,\n",
       " 'year': 34,\n",
       " 'work': 35,\n",
       " 'thing': 36,\n",
       " 'could': 37,\n",
       " 'plot': 38,\n",
       " 'say': 39,\n",
       " 'find': 40,\n",
       " 'really': 41,\n",
       " 'little': 42,\n",
       " 'show': 43,\n",
       " 'people': 44,\n",
       " 'man': 45,\n",
       " 'think': 46,\n",
       " 'never': 47,\n",
       " 'star': 48,\n",
       " 'love': 49,\n",
       " 'director': 50,\n",
       " 'great': 51,\n",
       " 'best': 52,\n",
       " 'new': 53,\n",
       " 'try': 54,\n",
       " 'performance': 55,\n",
       " 'big': 56,\n",
       " 'many': 57,\n",
       " 'action': 58,\n",
       " 'actor': 59,\n",
       " 'u': 60,\n",
       " 'want': 61,\n",
       " 'watch': 62,\n",
       " 'role': 63,\n",
       " 'another': 64,\n",
       " 'use': 65,\n",
       " 'back': 66,\n",
       " 'become': 67,\n",
       " 'audience': 68,\n",
       " 'world': 69,\n",
       " 'something': 70,\n",
       " 'still': 71,\n",
       " 'act': 72,\n",
       " 'day': 73,\n",
       " 'turn': 74,\n",
       " \"'re\": 75,\n",
       " 'however': 76,\n",
       " 'old': 77,\n",
       " 'set': 78,\n",
       " 'every': 79,\n",
       " 'though': 80,\n",
       " 'part': 81,\n",
       " 'comedy': 82,\n",
       " 'real': 83,\n",
       " 'funny': 84,\n",
       " 'guy': 85,\n",
       " 'begin': 86,\n",
       " 'enough': 87,\n",
       " 'around': 88,\n",
       " 'tell': 89,\n",
       " 'leave': 90,\n",
       " 'cast': 91,\n",
       " 'long': 92,\n",
       " 'point': 93,\n",
       " 'last': 94,\n",
       " 'may': 95,\n",
       " 'young': 96,\n",
       " 'fact': 97,\n",
       " 'run': 98,\n",
       " 'actually': 99,\n",
       " 'right': 100,\n",
       " 'woman': 101,\n",
       " 'script': 102,\n",
       " 'minute': 103,\n",
       " 'feel': 104,\n",
       " 'name': 105,\n",
       " 'almost': 106,\n",
       " \"'ve\": 107,\n",
       " 'write': 108,\n",
       " 'effect': 109,\n",
       " 'nothing': 110,\n",
       " 'john': 111,\n",
       " 'lot': 112,\n",
       " 'although': 113,\n",
       " 'place': 114,\n",
       " 'screen': 115,\n",
       " 'ever': 116,\n",
       " 'since': 117,\n",
       " 'moment': 118,\n",
       " 'start': 119,\n",
       " 'line': 120,\n",
       " 'live': 121,\n",
       " 'original': 122,\n",
       " 'kill': 123,\n",
       " 'call': 124,\n",
       " 'friend': 125,\n",
       " 'help': 126,\n",
       " 'lead': 127,\n",
       " 'family': 128,\n",
       " 'high': 129,\n",
       " 'without': 130,\n",
       " 'three': 131,\n",
       " 'problem': 132,\n",
       " 'keep': 133,\n",
       " 'picture': 134,\n",
       " 'least': 135,\n",
       " 'happen': 136,\n",
       " 'girl': 137,\n",
       " 'quite': 138,\n",
       " 'sequence': 139,\n",
       " 'away': 140,\n",
       " 'course': 141,\n",
       " 'ca': 142,\n",
       " 'need': 143,\n",
       " 'far': 144,\n",
       " 'might': 145,\n",
       " \"'m\": 146,\n",
       " 'rather': 147,\n",
       " 'must': 148,\n",
       " 'anything': 149,\n",
       " 'hard': 150,\n",
       " 'lose': 151,\n",
       " 'fall': 152,\n",
       " 'include': 153,\n",
       " 'laugh': 154,\n",
       " 'yet': 155,\n",
       " 'job': 156,\n",
       " 'child': 157,\n",
       " 'put': 158,\n",
       " 'american': 159,\n",
       " 'follow': 160,\n",
       " 'wife': 161,\n",
       " 'open': 162,\n",
       " 'alien': 163,\n",
       " 'kind': 164,\n",
       " 'hour': 165,\n",
       " 'always': 166,\n",
       " 'fun': 167,\n",
       " 'reason': 168,\n",
       " 'bit': 169,\n",
       " 'sense': 170,\n",
       " 'home': 171,\n",
       " 'special': 172,\n",
       " \"'ll\": 173,\n",
       " 'feature': 174,\n",
       " 'attempt': 175,\n",
       " 'hollywood': 176,\n",
       " 'instead': 177,\n",
       " 'bring': 178,\n",
       " 'human': 179,\n",
       " 'war': 180,\n",
       " 'head': 181,\n",
       " 'night': 182,\n",
       " 'series': 183,\n",
       " 'black': 184,\n",
       " 'half': 185,\n",
       " 'probably': 186,\n",
       " 'let': 187,\n",
       " 'hand': 188,\n",
       " 'along': 189,\n",
       " 'kid': 190,\n",
       " 'move': 191,\n",
       " 'pretty': 192,\n",
       " 'mean': 193,\n",
       " 'men': 194,\n",
       " 'everything': 195,\n",
       " 'meet': 196,\n",
       " 'mind': 197,\n",
       " 'involve': 198,\n",
       " 'dialogue': 199,\n",
       " 'sure': 200,\n",
       " 'idea': 201,\n",
       " 'together': 202,\n",
       " 'create': 203,\n",
       " 'face': 204,\n",
       " 'money': 205,\n",
       " 'believe': 206,\n",
       " 'interest': 207,\n",
       " 'father': 208,\n",
       " 'fight': 209,\n",
       " 'whole': 210,\n",
       " 'death': 211,\n",
       " 'horror': 212,\n",
       " 'force': 213,\n",
       " 'shot': 214,\n",
       " 'direct': 215,\n",
       " 'save': 216,\n",
       " 'everyone': 217,\n",
       " 'do': 218,\n",
       " 'city': 219,\n",
       " 'appear': 220,\n",
       " 'music': 221,\n",
       " 'sex': 222,\n",
       " 'question': 223,\n",
       " 'talk': 224,\n",
       " 'second': 225,\n",
       " 'couple': 226,\n",
       " 'perhaps': 227,\n",
       " 'small': 228,\n",
       " 'release': 229,\n",
       " 'less': 230,\n",
       " 'case': 231,\n",
       " 'eye': 232,\n",
       " 'next': 233,\n",
       " 'brother': 234,\n",
       " 'especially': 235,\n",
       " '10': 236,\n",
       " 'mother': 237,\n",
       " 'relationship': 238,\n",
       " 'expect': 239,\n",
       " 'word': 240,\n",
       " 'completely': 241,\n",
       " '2': 242,\n",
       " 'rest': 243,\n",
       " 'whose': 244,\n",
       " 'late': 245,\n",
       " 'evil': 246,\n",
       " 'die': 247,\n",
       " 'different': 248,\n",
       " 'boy': 249,\n",
       " 'simply': 250,\n",
       " 'care': 251,\n",
       " 'writer': 252,\n",
       " 'book': 253,\n",
       " 'anyone': 254,\n",
       " 'dead': 255,\n",
       " 'change': 256,\n",
       " 'base': 257,\n",
       " 'school': 258,\n",
       " 'michael': 259,\n",
       " 'several': 260,\n",
       " 'joke': 261,\n",
       " 'top': 262,\n",
       " 'sound': 263,\n",
       " 'review': 264,\n",
       " 'true': 265,\n",
       " 'humor': 266,\n",
       " 'matter': 267,\n",
       " 'town': 268,\n",
       " 'suppose': 269,\n",
       " 'entire': 270,\n",
       " 'group': 271,\n",
       " 'lack': 272,\n",
       " 'house': 273,\n",
       " 'add': 274,\n",
       " 'comic': 275,\n",
       " 'soon': 276,\n",
       " 'main': 277,\n",
       " \"'d\": 278,\n",
       " 'someone': 279,\n",
       " 'hit': 280,\n",
       " 'tv': 281,\n",
       " 'game': 282,\n",
       " 'wrong': 283,\n",
       " 'interesting': 284,\n",
       " 'mr': 285,\n",
       " 'fan': 286,\n",
       " 'full': 287,\n",
       " 'david': 288,\n",
       " 'side': 289,\n",
       " 'else': 290,\n",
       " 'present': 291,\n",
       " 'either': 292,\n",
       " 'ask': 293,\n",
       " 'final': 294,\n",
       " 'unfortunately': 295,\n",
       " 'car': 296,\n",
       " 'murder': 297,\n",
       " 'enjoy': 298,\n",
       " 'viewer': 299,\n",
       " 'james': 300,\n",
       " 'wonder': 301,\n",
       " 'element': 302,\n",
       " 'voice': 303,\n",
       " 'stop': 304,\n",
       " 'deal': 305,\n",
       " 'close': 306,\n",
       " 'credit': 307,\n",
       " 'often': 308,\n",
       " 'return': 309,\n",
       " 'later': 310,\n",
       " 'style': 311,\n",
       " 'camera': 312,\n",
       " 'break': 313,\n",
       " 'provide': 314,\n",
       " 'behind': 315,\n",
       " 'surprise': 316,\n",
       " 'certainly': 317,\n",
       " 'support': 318,\n",
       " 'person': 319,\n",
       " 'power': 320,\n",
       " 'son': 321,\n",
       " 'hero': 322,\n",
       " 'stand': 323,\n",
       " 'scream': 324,\n",
       " 'result': 325,\n",
       " 'despite': 326,\n",
       " 'team': 327,\n",
       " 'nice': 328,\n",
       " 'learn': 329,\n",
       " 'title': 330,\n",
       " 'finally': 331,\n",
       " 'perfect': 332,\n",
       " 'early': 333,\n",
       " 'order': 334,\n",
       " 'video': 335,\n",
       " 'maybe': 336,\n",
       " 'killer': 337,\n",
       " 'note': 338,\n",
       " 'robert': 339,\n",
       " 'summer': 340,\n",
       " 'piece': 341,\n",
       " 'able': 342,\n",
       " 'past': 343,\n",
       " 'consider': 344,\n",
       " 'fine': 345,\n",
       " 'experience': 346,\n",
       " 'miss': 347,\n",
       " 'daughter': 348,\n",
       " 'classic': 349,\n",
       " 'strong': 350,\n",
       " 'theater': 351,\n",
       " 'situation': 352,\n",
       " 'production': 353,\n",
       " 'example': 354,\n",
       " 'view': 355,\n",
       " 'event': 356,\n",
       " 'short': 357,\n",
       " 'sort': 358,\n",
       " 'thriller': 359,\n",
       " 'white': 360,\n",
       " 'kevin': 361,\n",
       " 'deep': 362,\n",
       " 'joe': 363,\n",
       " 'realize': 364,\n",
       " 'talent': 365,\n",
       " 'dog': 366,\n",
       " 'hold': 367,\n",
       " 'body': 368,\n",
       " 'worth': 369,\n",
       " 'dark': 370,\n",
       " 'earth': 371,\n",
       " 'drama': 372,\n",
       " 'decide': 373,\n",
       " 'version': 374,\n",
       " 'level': 375,\n",
       " 'self': 376,\n",
       " 'light': 377,\n",
       " 'room': 378,\n",
       " 'heart': 379,\n",
       " 'nearly': 380,\n",
       " 'upon': 381,\n",
       " 'offer': 382,\n",
       " 'spend': 383,\n",
       " 'up': 384,\n",
       " 'plan': 385,\n",
       " 'violence': 386,\n",
       " 'major': 387,\n",
       " 'screenplay': 388,\n",
       " 'throughout': 389,\n",
       " 'cut': 390,\n",
       " 'cop': 391,\n",
       " 'hope': 392,\n",
       " 'manage': 393,\n",
       " 'beautiful': 394,\n",
       " 'ship': 395,\n",
       " 'direction': 396,\n",
       " 'figure': 397,\n",
       " 'exactly': 398,\n",
       " 'art': 399,\n",
       " 'computer': 400,\n",
       " 'jack': 401,\n",
       " 'dream': 402,\n",
       " 'obvious': 403,\n",
       " 'fail': 404,\n",
       " 'disney': 405,\n",
       " 'already': 406,\n",
       " 'others': 407,\n",
       " 'genre': 408,\n",
       " 'simple': 409,\n",
       " 'state': 410,\n",
       " 'entertain': 411,\n",
       " 'battle': 412,\n",
       " 'age': 413,\n",
       " 'five': 414,\n",
       " 'four': 415,\n",
       " 'guess': 416,\n",
       " 'space': 417,\n",
       " 'jackie': 418,\n",
       " 'king': 419,\n",
       " 'pay': 420,\n",
       " 'wait': 421,\n",
       " 'career': 422,\n",
       " 'sometimes': 423,\n",
       " 'flick': 424,\n",
       " 'form': 425,\n",
       " 'member': 426,\n",
       " 'novel': 427,\n",
       " 'sit': 428,\n",
       " 'number': 429,\n",
       " 'truly': 430,\n",
       " 'oscar': 431,\n",
       " 'hear': 432,\n",
       " 'pull': 433,\n",
       " 'waste': 434,\n",
       " 'tom': 435,\n",
       " 'husband': 436,\n",
       " '1': 437,\n",
       " 'read': 438,\n",
       " 'drive': 439,\n",
       " 'filmmaker': 440,\n",
       " 'chase': 441,\n",
       " 'lee': 442,\n",
       " 'allow': 443,\n",
       " 'type': 444,\n",
       " 'easy': 445,\n",
       " 'peter': 446,\n",
       " 'york': 447,\n",
       " 'touch': 448,\n",
       " 'god': 449,\n",
       " 'walk': 450,\n",
       " 'drug': 451,\n",
       " 'stay': 452,\n",
       " 'planet': 453,\n",
       " 'fiction': 454,\n",
       " 'the': 455,\n",
       " 'wo': 456,\n",
       " 'explain': 457,\n",
       " 'stupid': 458,\n",
       " 'parent': 459,\n",
       " 'sequel': 460,\n",
       " 'rise': 461,\n",
       " 'yes': 462,\n",
       " 'shoot': 463,\n",
       " 'saw': 464,\n",
       " 'fill': 465,\n",
       " 'quickly': 466,\n",
       " 'song': 467,\n",
       " 'tale': 468,\n",
       " 'twist': 469,\n",
       " 'romantic': 470,\n",
       " 'carry': 471,\n",
       " 'possible': 472,\n",
       " 'remember': 473,\n",
       " 'score': 474,\n",
       " 'remain': 475,\n",
       " 'chance': 476,\n",
       " 'bore': 477,\n",
       " 'extremely': 478,\n",
       " 'prove': 479,\n",
       " 'material': 480,\n",
       " 'detail': 481,\n",
       " 'mostly': 482,\n",
       " 'attention': 483,\n",
       " 'future': 484,\n",
       " 'gun': 485,\n",
       " 'single': 486,\n",
       " 'particularly': 487,\n",
       " 'de': 488,\n",
       " 'catch': 489,\n",
       " 'charm': 490,\n",
       " 'project': 491,\n",
       " 'paul': 492,\n",
       " 'quality': 493,\n",
       " 'police': 494,\n",
       " 'none': 495,\n",
       " 'co': 496,\n",
       " 'van': 497,\n",
       " 'hell': 498,\n",
       " 'eventually': 499,\n",
       " 'escape': 500,\n",
       " 'throw': 501,\n",
       " 'wild': 502,\n",
       " 'emotional': 503,\n",
       " 'science': 504,\n",
       " 'mention': 505,\n",
       " 'dr': 506,\n",
       " 'crime': 507,\n",
       " 'image': 508,\n",
       " 'grow': 509,\n",
       " 'out': 510,\n",
       " 'focus': 511,\n",
       " 'smith': 512,\n",
       " 'villain': 513,\n",
       " 'slow': 514,\n",
       " 'girlfriend': 515,\n",
       " 'mark': 516,\n",
       " 'alone': 517,\n",
       " 'win': 518,\n",
       " 'george': 519,\n",
       " 'rock': 520,\n",
       " 'obviously': 521,\n",
       " 'television': 522,\n",
       " 'send': 523,\n",
       " 'within': 524,\n",
       " 'in': 525,\n",
       " 'usually': 526,\n",
       " 'success': 527,\n",
       " 'among': 528,\n",
       " 'million': 529,\n",
       " 'premise': 530,\n",
       " 'middle': 531,\n",
       " 'large': 532,\n",
       " 'stuff': 533,\n",
       " 'poor': 534,\n",
       " '3': 535,\n",
       " 'complete': 536,\n",
       " 'speak': 537,\n",
       " 'across': 538,\n",
       " 'low': 539,\n",
       " 'except': 540,\n",
       " 'chris': 541,\n",
       " 'theme': 542,\n",
       " 'secret': 543,\n",
       " 'happy': 544,\n",
       " 'mission': 545,\n",
       " 'build': 546,\n",
       " 'water': 547,\n",
       " 'reality': 548,\n",
       " 'history': 549,\n",
       " 'steal': 550,\n",
       " 'whether': 551,\n",
       " 'subject': 552,\n",
       " 'wonderful': 553,\n",
       " 'local': 554,\n",
       " 'crew': 555,\n",
       " 'serious': 556,\n",
       " 'trouble': 557,\n",
       " 'oh': 558,\n",
       " 'important': 559,\n",
       " 'box': 560,\n",
       " 'motion': 561,\n",
       " 'effort': 562,\n",
       " 'near': 563,\n",
       " 'ryan': 564,\n",
       " 'understand': 565,\n",
       " 'agent': 566,\n",
       " 'develop': 567,\n",
       " 'impressive': 568,\n",
       " 'cool': 569,\n",
       " 'law': 570,\n",
       " 'america': 571,\n",
       " 'cover': 572,\n",
       " 'office': 573,\n",
       " 'entertainment': 574,\n",
       " 'aspect': 575,\n",
       " 'recent': 576,\n",
       " 'robin': 577,\n",
       " 'actress': 578,\n",
       " 'basically': 579,\n",
       " 'cause': 580,\n",
       " 'apparently': 581,\n",
       " 'studio': 582,\n",
       " 'easily': 583,\n",
       " 'williams': 584,\n",
       " 'screenwriter': 585,\n",
       " 'message': 586,\n",
       " 'mystery': 587,\n",
       " 'william': 588,\n",
       " 'somehow': 589,\n",
       " 'thought': 590,\n",
       " 'party': 591,\n",
       " 'bill': 592,\n",
       " 'pick': 593,\n",
       " 'discover': 594,\n",
       " 'lie': 595,\n",
       " 'wish': 596,\n",
       " 'fast': 597,\n",
       " 'street': 598,\n",
       " 'jones': 599,\n",
       " 'fear': 600,\n",
       " 'pace': 601,\n",
       " 'suspense': 602,\n",
       " 'doubt': 603,\n",
       " 'rich': 604,\n",
       " 'red': 605,\n",
       " 'batman': 606,\n",
       " 'smart': 607,\n",
       " 'straight': 608,\n",
       " 'attack': 609,\n",
       " 'difficult': 610,\n",
       " 'blood': 611,\n",
       " 'ago': 612,\n",
       " 'flaw': 613,\n",
       " 'certain': 614,\n",
       " 'romance': 615,\n",
       " 'producer': 616,\n",
       " 'country': 617,\n",
       " 'ben': 618,\n",
       " 'hilarious': 619,\n",
       " 'business': 620,\n",
       " 'presence': 621,\n",
       " 'popular': 622,\n",
       " 'effective': 623,\n",
       " 'fly': 624,\n",
       " 'due': 625,\n",
       " 'convince': 626,\n",
       " 'critic': 627,\n",
       " 'company': 628,\n",
       " 'approach': 629,\n",
       " 'clear': 630,\n",
       " 'answer': 631,\n",
       " 'annoy': 632,\n",
       " 'appeal': 633,\n",
       " 'scary': 634,\n",
       " 'dramatic': 635,\n",
       " 'general': 636,\n",
       " 'vampire': 637,\n",
       " 'forget': 638,\n",
       " 'produce': 639,\n",
       " 'draw': 640,\n",
       " 'class': 641,\n",
       " 'strange': 642,\n",
       " 'budget': 643,\n",
       " 'sexual': 644,\n",
       " 'fire': 645,\n",
       " 'surprisingly': 646,\n",
       " 'control': 647,\n",
       " 'anyway': 648,\n",
       " '4': 649,\n",
       " 'personal': 650,\n",
       " 'continue': 651,\n",
       " 'adult': 652,\n",
       " 'somewhat': 653,\n",
       " 'emotion': 654,\n",
       " 'harry': 655,\n",
       " 'third': 656,\n",
       " 'ability': 657,\n",
       " 'previous': 658,\n",
       " 'jim': 659,\n",
       " 'successful': 660,\n",
       " 'prison': 661,\n",
       " 'similar': 662,\n",
       " 'absolutely': 663,\n",
       " 'former': 664,\n",
       " 'deliver': 665,\n",
       " 'choice': 666,\n",
       " 'share': 667,\n",
       " 'rule': 668,\n",
       " 'sister': 669,\n",
       " 'student': 670,\n",
       " 'familiar': 671,\n",
       " 'intelligent': 672,\n",
       " 'excellent': 673,\n",
       " 'becomes': 674,\n",
       " 'date': 675,\n",
       " 'bob': 676,\n",
       " 'towards': 677,\n",
       " 'predictable': 678,\n",
       " 'powerful': 679,\n",
       " 'cinema': 680,\n",
       " 'giant': 681,\n",
       " 'marry': 682,\n",
       " 'teen': 683,\n",
       " 'beyond': 684,\n",
       " 'visual': 685,\n",
       " 'victim': 686,\n",
       " 'nature': 687,\n",
       " 'reveal': 688,\n",
       " 'serve': 689,\n",
       " 'r': 690,\n",
       " 'trailer': 691,\n",
       " 'b': 692,\n",
       " 'sam': 693,\n",
       " 'clever': 694,\n",
       " 'martin': 695,\n",
       " 'definitely': 696,\n",
       " 'and': 697,\n",
       " 'felt': 698,\n",
       " 'usual': 699,\n",
       " 'brilliant': 700,\n",
       " 'murphy': 701,\n",
       " 'musical': 702,\n",
       " 'stone': 703,\n",
       " 'blue': 704,\n",
       " 'free': 705,\n",
       " 'park': 706,\n",
       " 'bunch': 707,\n",
       " 'solid': 708,\n",
       " 'reach': 709,\n",
       " 'wear': 710,\n",
       " 'mess': 711,\n",
       " 'rating': 712,\n",
       " 'appearance': 713,\n",
       " 'sweet': 714,\n",
       " 'off': 715,\n",
       " 'dance': 716,\n",
       " 'potential': 717,\n",
       " 'seriously': 718,\n",
       " 'master': 719,\n",
       " 'huge': 720,\n",
       " 'promise': 721,\n",
       " 'track': 722,\n",
       " 'suspect': 723,\n",
       " 'destroy': 724,\n",
       " 'strike': 725,\n",
       " 'bond': 726,\n",
       " 'shock': 727,\n",
       " 'choose': 728,\n",
       " 'week': 729,\n",
       " 'land': 730,\n",
       " 'to': 731,\n",
       " 'favorite': 732,\n",
       " 'feeling': 733,\n",
       " 'search': 734,\n",
       " 'unlike': 735,\n",
       " 'amount': 736,\n",
       " 'l': 737,\n",
       " 'non': 738,\n",
       " 'perfectly': 739,\n",
       " 'ex': 740,\n",
       " 'travel': 741,\n",
       " 'decent': 742,\n",
       " 'likely': 743,\n",
       " 'adventure': 744,\n",
       " 'tone': 745,\n",
       " 'scott': 746,\n",
       " 'e': 747,\n",
       " 'join': 748,\n",
       " 'issue': 749,\n",
       " 'enjoyable': 750,\n",
       " 'immediately': 751,\n",
       " 'sign': 752,\n",
       " 'private': 753,\n",
       " 'monster': 754,\n",
       " 'treat': 755,\n",
       " 'frank': 756,\n",
       " 'cameron': 757,\n",
       " 'truman': 758,\n",
       " 'door': 759,\n",
       " 'ten': 760,\n",
       " 'train': 761,\n",
       " 'bruce': 762,\n",
       " 'air': 763,\n",
       " 'la': 764,\n",
       " 'dumb': 765,\n",
       " 'deserve': 766,\n",
       " 'handle': 767,\n",
       " 'cold': 768,\n",
       " 'overall': 769,\n",
       " 'cross': 770,\n",
       " 'inside': 771,\n",
       " 'capture': 772,\n",
       " 'impossible': 773,\n",
       " 'carter': 774,\n",
       " 'contain': 775,\n",
       " 'richard': 776,\n",
       " 'rat': 777,\n",
       " 'trek': 778,\n",
       " 'purpose': 779,\n",
       " 'merely': 780,\n",
       " 'stick': 781,\n",
       " 'race': 782,\n",
       " 'neither': 783,\n",
       " 'hop': 784,\n",
       " 'particular': 785,\n",
       " 'list': 786,\n",
       " 'ultimately': 787,\n",
       " 'depth': 788,\n",
       " 'step': 789,\n",
       " 'struggle': 790,\n",
       " 'modern': 791,\n",
       " 'truth': 792,\n",
       " 'silly': 793,\n",
       " 'club': 794,\n",
       " 'otherwise': 795,\n",
       " 'mar': 796,\n",
       " 'society': 797,\n",
       " 'sight': 798,\n",
       " 'tim': 799,\n",
       " 'term': 800,\n",
       " 'allen': 801,\n",
       " 'personality': 802,\n",
       " 'player': 803,\n",
       " 'government': 804,\n",
       " 'describe': 805,\n",
       " 'of': 806,\n",
       " 'political': 807,\n",
       " 'various': 808,\n",
       " 'talented': 809,\n",
       " 'opportunity': 810,\n",
       " 'west': 811,\n",
       " 'design': 812,\n",
       " 'development': 813,\n",
       " 'key': 814,\n",
       " 'succeed': 815,\n",
       " 'soundtrack': 816,\n",
       " 'female': 817,\n",
       " 'english': 818,\n",
       " 'army': 819,\n",
       " 'bear': 820,\n",
       " 'slightly': 821,\n",
       " 'cliche': 822,\n",
       " 'heavy': 823,\n",
       " 'steve': 824,\n",
       " '5': 825,\n",
       " 'pop': 826,\n",
       " 'buy': 827,\n",
       " 'month': 828,\n",
       " 'gag': 829,\n",
       " 'toy': 830,\n",
       " 'foot': 831,\n",
       " 'tension': 832,\n",
       " 'six': 833,\n",
       " 'beat': 834,\n",
       " 'compare': 835,\n",
       " 'lover': 836,\n",
       " 'grace': 837,\n",
       " 'hate': 838,\n",
       " 'disaster': 839,\n",
       " 'soldier': 840,\n",
       " 'drop': 841,\n",
       " 'eddie': 842,\n",
       " 'raise': 843,\n",
       " 'kiss': 844,\n",
       " 'pass': 845,\n",
       " 'over': 846,\n",
       " 'imagine': 847,\n",
       " 'chan': 848,\n",
       " 'today': 849,\n",
       " 'engage': 850,\n",
       " 'memorable': 851,\n",
       " 'max': 852,\n",
       " 'detective': 853,\n",
       " 'hill': 854,\n",
       " 'baby': 855,\n",
       " 'confuse': 856,\n",
       " 'require': 857,\n",
       " 'totally': 858,\n",
       " 'introduce': 859,\n",
       " 'background': 860,\n",
       " 'color': 861,\n",
       " 'roll': 862,\n",
       " 'front': 863,\n",
       " 'fi': 864,\n",
       " 'ape': 865,\n",
       " 'woody': 866,\n",
       " 'award': 867,\n",
       " 'wood': 868,\n",
       " 'animation': 869,\n",
       " 'leader': 870,\n",
       " 'machine': 871,\n",
       " 'rescue': 872,\n",
       " 'cheap': 873,\n",
       " 'thrill': 874,\n",
       " 'excite': 875,\n",
       " 'terrible': 876,\n",
       " 'doctor': 877,\n",
       " 'simon': 878,\n",
       " 'sci': 879,\n",
       " 'mary': 880,\n",
       " 'angel': 881,\n",
       " 'blow': 882,\n",
       " 'entirely': 883,\n",
       " 'jump': 884,\n",
       " 'steven': 885,\n",
       " 'actual': 886,\n",
       " 'creature': 887,\n",
       " 'british': 888,\n",
       " 'constantly': 889,\n",
       " 'quick': 890,\n",
       " 'standard': 891,\n",
       " 'double': 892,\n",
       " 'queen': 893,\n",
       " 'impact': 894,\n",
       " 'fantasy': 895,\n",
       " 'brief': 896,\n",
       " 'ridiculous': 897,\n",
       " 'cinematography': 898,\n",
       " 'typical': 899,\n",
       " 'nick': 900,\n",
       " 'tough': 901,\n",
       " 'officer': 902,\n",
       " 'atmosphere': 903,\n",
       " 'occur': 904,\n",
       " 'ground': 905,\n",
       " 'minor': 906,\n",
       " 'seven': 907,\n",
       " 'trip': 908,\n",
       " 'notice': 909,\n",
       " 'violent': 910,\n",
       " 'subtle': 911,\n",
       " 'beauty': 912,\n",
       " '8': 913,\n",
       " 'dollar': 914,\n",
       " 'fairly': 915,\n",
       " 'road': 916,\n",
       " 'island': 917,\n",
       " 'on': 918,\n",
       " 'ride': 919,\n",
       " 'admit': 920,\n",
       " 'suffer': 921,\n",
       " 'dull': 922,\n",
       " 'highly': 923,\n",
       " 'rush': 924,\n",
       " 'partner': 925,\n",
       " 'suddenly': 926,\n",
       " 'willis': 927,\n",
       " 'count': 928,\n",
       " 'survive': 929,\n",
       " 'boring': 930,\n",
       " 'fit': 931,\n",
       " 'grant': 932,\n",
       " 'store': 933,\n",
       " 'complex': 934,\n",
       " 'president': 935,\n",
       " 'college': 936,\n",
       " 'portray': 937,\n",
       " 'hank': 938,\n",
       " 'animal': 939,\n",
       " 'ii': 940,\n",
       " 'concept': 941,\n",
       " 'whatever': 942,\n",
       " 'bug': 943,\n",
       " 'indeed': 944,\n",
       " 'flat': 945,\n",
       " 'mike': 946,\n",
       " 'costume': 947,\n",
       " 'episode': 948,\n",
       " 'menace': 949,\n",
       " 'basic': 950,\n",
       " 'recently': 951,\n",
       " 'outside': 952,\n",
       " 'plenty': 953,\n",
       " 'godzilla': 954,\n",
       " 'brown': 955,\n",
       " 'titanic': 956,\n",
       " 'suit': 957,\n",
       " 'cute': 958,\n",
       " 'respect': 959,\n",
       " 'cinematic': 960,\n",
       " 'climax': 961,\n",
       " 'meanwhile': 962,\n",
       " 'hot': 963,\n",
       " 'awful': 964,\n",
       " 'common': 965,\n",
       " 'edit': 966,\n",
       " 'clearly': 967,\n",
       " 'x': 968,\n",
       " 'suggest': 969,\n",
       " 'crash': 970,\n",
       " 're': 971,\n",
       " 'century': 972,\n",
       " 'conclusion': 973,\n",
       " '90': 974,\n",
       " 'sean': 975,\n",
       " 'arrive': 976,\n",
       " 'believable': 977,\n",
       " 'arm': 978,\n",
       " 'lawyer': 979,\n",
       " 'recommend': 980,\n",
       " 'chemistry': 981,\n",
       " 'band': 982,\n",
       " 'possibly': 983,\n",
       " 'thin': 984,\n",
       " 'encounter': 985,\n",
       " 'jerry': 986,\n",
       " 'scientist': 987,\n",
       " 'realistic': 988,\n",
       " 'somewhere': 989,\n",
       " 'avoid': 990,\n",
       " 'carrey': 991,\n",
       " 'french': 992,\n",
       " 'twenty': 993,\n",
       " 'fashion': 994,\n",
       " 'buddy': 995,\n",
       " 'language': 996,\n",
       " 'period': 997,\n",
       " 'witch': 998,\n",
       " 'aside': 999,\n",
       " 'haunt': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(tokenizer.word_index)\n",
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting text data to abv to integer values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 289 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "seq = tokenizer.texts_to_sequences(df_preprocessed[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"happy bastard 's quick movie review damn y2k bug 's got head start movie star jamie lee curtis another baldwin brother william time story regard crew tugboat come across deserted russian tech ship strangeness kick power back little know power within go gore bring action sequence virus still feel empty like movie go flash substance n't know crew really middle nowhere n't know origin take ship big pink flashy thing hit mir course n't know donald sutherland stumble around drunkenly throughout 's `` hey let 's chase people around robot `` acting average even like curtis 're likely get kick work halloween h20 sutherland waste baldwin well 's act like baldwin course real star stan winston 's robot design schnazzy cgi occasional good gore shot like pick someone 's brain robot body part really turn 's movie otherwise 's pretty much sunken ship movie\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_preprocessed['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 544, 2388,    1,  890,    3,  264, 1095,  943,    1, 4785,  181,\n",
       "        119,    3,   48, 2666,  442, 2961,   64, 1400,  234,  588,   12,\n",
       "         17, 1847,  555,   23,  538, 1541, 3416,  395, 1022,  320,   66,\n",
       "         42,   27,  320,  524,   11, 1153,  178,   58,  139, 1560,   71,\n",
       "        104, 1783,    7,    3,   11, 1414, 1615,    4,   27,  555,   41,\n",
       "        531, 1289,    4,   27, 6713,   20,  395,   56, 4198, 2667,   36,\n",
       "        280, 9313,  141,    4,   27, 2886, 2814, 1542,   88,  389,    1,\n",
       "       1742,  187,    1,  441,   44,   88, 1802, 1561, 1063,   14,    7,\n",
       "       2961,   75,  743,    9, 1022,   35, 2094, 8609, 2814,  434, 1400,\n",
       "         16,    1,   72,    7, 1400,  141,   83,   48, 6714,    1, 1802,\n",
       "        812, 2294, 1923,   13, 1153,  214,    7,  593,  279,    1, 1023,\n",
       "       1802,  368,   81,   41,   74,    1,    3,  795,    1,  192,   21,\n",
       "        395,    3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "WE CAN SEE THE DIFFERENCE 10 , BCOZ OTHER VALUES ARE GRATER THEN 10000 SO ITS NOT CONSIDERED\n",
    "'''\n",
    "print(df_preprocessed['word_count'][1])\n",
    "print(len(np.array(seq[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Actual VOCAB_SIZE deined:  10000\n",
      " Actual tokens created:  34094\n"
     ]
    }
   ],
   "source": [
    "print(\" Actual VOCAB_SIZE deined: \",VOCAB_SIZE)\n",
    "print(\" Actual tokens created: \",len(tokenizer.word_index))# IF WE WANT TO CONSIDER EVERY TEXT IN VOCAB PASS THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and Truncating Data\n",
    "* To feed it to N.N, inputs to have the same length\n",
    " - Either we ensure that all sequences in the entire data-set have the same length\n",
    " - Or Entier batch should be of same length\n",
    "* Going about choosing ampunt to pad\n",
    " - going with longest seq, would be just waste of memory for texr whose length is small\n",
    " - going with smalles seq , would be just ignoring other imp values \n",
    " - so we go optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 5595 6919  146   11  434  240]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "All that the Embedding layer does is to map the integer inputs to the vectors found at the corresponding index in the \n",
    "embedding matrix, i.e. the sequence [1, 2] would be converted to [embeddings[1], embeddings[2]]\n",
    "'''\n",
    "print(np.array(seq[506]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding and trucating here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data_pad = sequence.pad_sequences(seq , maxlen=INPUT_TEXT_LENGTH,padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 695)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data_pad.shape\n",
    "# 2k review\n",
    "# and 695 fixed I/P shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking\n",
    "#imdb_data_pad[4]\n",
    "type(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x22602f3eb00>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Inverse Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    \n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"film extraordinarily horrendous 'm go waste word\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_to_string(seq[506])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34094"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N.N Model\n",
    "len(tokenizer.word_index) # This are total word in dict\n",
    "#it depends if i want to use entier dict or only few occuring word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data to test/ train / dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= imdb_data_pad #PADDED VERSION OF DATA\n",
    "y= df_preprocessed['sentiment'] # LABELS OF DATA\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef export_test_train_data_to_h5(filename, X_train, X_test, y_train, y_test):\\n    #filename has to be xyzw.h5 with extension\\n    h5f = h5py.File(filename, \\'w\\')\\n    h5f.create_dataset(\\'X\\', data=X)           #np.ndarry\\n    h5f.create_dataset(\\'y\\', data=np.array(y)) #\\n    h5f.create_dataset(\\'X_train\\', data=X_train)           #np.ndarry\\n    h5f.create_dataset(\\'y_train\\', data=np.array(y_train)) #pd df\\n    h5f.create_dataset(\\'X_test\\', data=X_test)\\n    h5f.create_dataset(\\'y_test\\', data=np.array(y_test))\\n    # export weights of embedding_matrix incase of  \\n    h5f.close()\\n    print(\"file created\")\\n\\nfilename = \"2_2_keras.h5\"\\nexport_test_train_data_to_h5(filename,X_train, X_test, y_train, y_test)\\n\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def export_test_train_data_to_h5(filename, X_train, X_test, y_train, y_test):\n",
    "    #filename has to be xyzw.h5 with extension\n",
    "    h5f = h5py.File(filename, 'w')\n",
    "    h5f.create_dataset('X', data=X)           #np.ndarry\n",
    "    h5f.create_dataset('y', data=np.array(y)) #\n",
    "    h5f.create_dataset('X_train', data=X_train)           #np.ndarry\n",
    "    h5f.create_dataset('y_train', data=np.array(y_train)) #pd df\n",
    "    h5f.create_dataset('X_test', data=X_test)\n",
    "    h5f.create_dataset('y_test', data=np.array(y_test))\n",
    "    # export weights of embedding_matrix incase of  \n",
    "    h5f.close()\n",
    "    print(\"file created\")\n",
    "\n",
    "filename = \"2_2_keras.h5\"\n",
    "export_test_train_data_to_h5(filename,X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING A MODEL\n",
    "* http://fizzylogic.nl/2017/05/08/monitor-progress-of-your-keras-based-neural-network-using-tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 695, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 695, 16)           5664      \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 695, 8)            624       \n",
      "_________________________________________________________________\n",
      "cu_dnngru_3 (CuDNNGRU)       (None, 4)                 168       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 1,006,461\n",
      "Trainable params: 1,006,461\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Wall time: 9.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Sequential()\n",
    "                   #vocab_size\n",
    "model.add(Embedding(VOCAB_SIZE, 100, input_length=INPUT_TEXT_LENGTH))\n",
    "\n",
    "'''\n",
    "model.add(Dropout(0.2))\n",
    "'''\n",
    "model.add(CuDNNGRU(units=16, return_sequences=True))\n",
    "model.add(CuDNNGRU(units=8, return_sequences=True))\n",
    "model.add(CuDNNGRU(units=4))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print out model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "import os\n",
    "#install graph viz locally 1st\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'# install \n",
    "plot_model(model, to_file='3_3_GRU_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " - 4s - loss: 0.6930 - acc: 0.5149\n",
      "Epoch 2/3\n",
      " - 2s - loss: 0.6667 - acc: 0.7955\n",
      "Epoch 3/3\n",
      " - 2s - loss: 0.5289 - acc: 0.8896\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=64, verbose=2,callbacks=[tensorboard]) \n",
    "# use validation_split=0.08 only incase data is not been splitte earlier\n",
    "\n",
    "'''\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "for train_indices, test_indices in kf.split(X):\n",
    "    clf.fit(X[train_indices], y[train_indices])\n",
    "    print(clf.score(X[test_indices], y[test_indices]))\n",
    "'''\n",
    "\n",
    "# TENSORBOARD\n",
    "'''\n",
    "# tensorboard --logdir logs/\n",
    "# http://localhost:6006/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.15%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Neural networks are stochastic, they can produce different results when the same model is fit on the same data.\n",
    "This is mainly because of the random initial weights and the shuffling of patterns during mini-batch gradient descent. \n",
    "This means that any one scoring of a model is unreliable and we should estimate model skill based on an average of multiple runs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-fa8cef49474b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Have to try:\n",
    "* include pre trained embedding before\n",
    "* set trainable param**\n",
    "* other architecture\n",
    "* hyperparam settings etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refs\n",
    "#use tensorboard\n",
    "# 1. https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "# 2. Logloss is a very useful measure for evaluating the performance of learning algorithms on multi-class classification problems:\n",
    "# 3. http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'keras_preprocessing.text.Tokenizer'>\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import pickle\n",
    "\n",
    "with open('tokenizer_instance.pickle', 'rb') as handle:\n",
    "    prod_instance = pickle.load(handle)\n",
    "\n",
    "print(type(prod_instance))\n",
    "\n",
    "'''\n",
    "\n",
    "#result = text_to_word_sequence(df_preprocessed['review'][0])\n",
    "\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'really', 'sucks', 'can', 'i', 'get', 'my', 'money', 'back', 'please']\n",
      "[[], [3], [41], [], [5078], [3034], [9], [], [205], [66], [1068]]\n"
     ]
    }
   ],
   "source": [
    "result = text_to_word_sequence(\"This movie really sucks! Can I get my money back please\")\n",
    "\n",
    "print(result)\n",
    "text_to_int= prod_instance.texts_to_sequences(result)\n",
    "print(text_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 41, 5078, 3034, 9, 205, 66, 1068]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import functools.reduce\n",
    "reduce(lambda x,y :x+y , xx)\"\"\"\n",
    "print(sum(text_to_int,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(text_to_int)\n",
    "lst = [[3, 41, 5078, 3034, 9, 205, 66, 1068]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    3   41\n",
      "  5078 3034    9  205   66 1068]]\n"
     ]
    }
   ],
   "source": [
    "#data = sequence.pad_sequences(lst , maxlen=INPUT_TEXT_LENGTH,padding='pre', truncating='pre')\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# define sequences\n",
    "sequences =[ [1, 2, 3] ]\n",
    "\n",
    "padded = sequence.pad_sequences(lst , maxlen=20,padding='pre', truncating='pre')\n",
    "print(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
