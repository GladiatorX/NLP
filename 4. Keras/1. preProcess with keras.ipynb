{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# text_to_word_sequence()  aka  tokenizing sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "c:\\users\\me\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'he', \"couldn't\", 'drive', 'green', 'blueish']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = \"The quick brown fox jumped over the lazy dog. He couldn't drive green-blueish!\"\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_hot encoding  (converts to integer equiv.)\n",
    "* Name suggests that it will create a one-hot encoding of the document, WHICH IS NOT THE CASE.\n",
    "* uses HASH function means that there may be collisions, hence tokens wont be unique (eg: 2 diffrent word in vocab may be assigned same integer token ) so ideally vocabulary size is perhaps 25% more to minimize the number of collisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  14\n",
      "[1, 7, 2, 5, 17, 12, 1, 3, 11, 7, 13, 5, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "#  text lower case, filter out punctuation, and split words based on white space\n",
    "# also need vocab size = defines the hashing space from which words are hashed\n",
    "# recommended vocabulary by some percentage (perhaps 25%) to minimize the number of collisions\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.text import one_hot\n",
    "\n",
    "# define the document\n",
    "text = \"The quick brown fox jumped over the lazy dog. He couldn't drive green-blueish!\"\n",
    "\n",
    "# tokenize the document\n",
    "tokensList = text_to_word_sequence(text)\n",
    "vocab_size = len(tokensList)\n",
    "print(\"vocab_size: \", vocab_size)\n",
    "\n",
    "\n",
    "encoded = one_hot(text, round(vocab_size*1.3)) # or add extra value\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer API\n",
    "* okenizer must be constructed and then fit on either raw text documents or integer encoded text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "\t\t'Good work',\n",
    "\t\t'Great effort',\n",
    "\t\t'nice work',\n",
    "\t\t'Excellent!']\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()# HERE WE CAN PASS VOCAB, SO WHEN CONVERTIN TO SEQ; IT WOULD IGNORE ALL VALUES ABOVE THAT DEFINED VOCAB \n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# has 4 attributes \n",
    "* word_counts: A dictionary of words and their counts.\n",
    "* word_docs: A dictionary of words and how many documents each appeared in.\n",
    "* word_index: A dictionary of words and their uniquely assigned integers.\n",
    "* document_count:An integer count of the total number of documents that were used to fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('well', 1),\n",
       "             ('done', 1),\n",
       "             ('good', 1),\n",
       "             ('work', 2),\n",
       "             ('great', 1),\n",
       "             ('effort', 1),\n",
       "             ('nice', 1),\n",
       "             ('excellent', 1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'done': 1,\n",
       "             'well': 1,\n",
       "             'work': 2,\n",
       "             'good': 1,\n",
       "             'great': 1,\n",
       "             'effort': 1,\n",
       "             'nice': 1,\n",
       "             'excellent': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': 1,\n",
       " 'well': 2,\n",
       " 'done': 3,\n",
       " 'good': 4,\n",
       " 'great': 5,\n",
       " 'effort': 6,\n",
       " 'nice': 7,\n",
       " 'excellent': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index\n",
    "#oredered as per max. freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [4, 1], [5, 6], [7, 1], [8]]\n"
     ]
    }
   ],
   "source": [
    "seq = tokenizer.texts_to_sequences(docs)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
