{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. global variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IN MAIN AS WELL AS DEPLOYABLE SCRIPT\n",
    "INPUT_TEXT_LENGTH = 695\n",
    "FILENAME = \" \"\n",
    "PROGNAME = \" \" # use it has pre fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving tokenizer instace so word can be converted to int tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "import pickle\n",
    "\n",
    "# in training notebook [finally] after model is good to be deployed\n",
    "with open(FILENAME,'_tokenizer_instance.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "print(type(prod_instance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving keras model n weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(FILENAME,\"_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(FILENAME,\"_weight.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # TO DO @ Deployable stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing tokenizer & checking Vectorizing new inputs \n",
    "* store tokenizer @ training and retrive its values @ deployment \n",
    "* its working illustration 5.mlmastery @@ Vectorising KERAS - GRU.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''its working illustration 5.mlmastery @@ Vectorising KERAS - GRU.ipynb'''\n",
    "import pickle\n",
    "  \n",
    "    \n",
    "# in deployement script    \n",
    "#iniitialize it in the deployement script along with the .pickle file @ specified location\n",
    "with open(FILENAME,'_tokenizer_instance.pickle', 'rb') as handle:\n",
    "    prod_instance_tokenizer = pickle.load(handle)\n",
    "\n",
    "print(type(prod_instance))\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer,text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "result = text_to_word_sequence(\"here will be the new text\")     # converting it to tokens\n",
    "word_to_int= prod_instance_tokenizer.texts_to_sequences(result) # mapping it to int value from vocab\n",
    "text_to_int = sum(word_to_int,[])\n",
    "\n",
    "#ONLY PADDING IS REMAINIG\n",
    "text_to_int = [ text_to_int ]\n",
    "\n",
    "padded = sequence.pad_sequences(seq , maxlen=INPUT_TEXT_LENGTH,padding='pre', truncating='pre')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. preprocessing - total steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre process step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie really sucks get money back please\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "punct = set(string.punctuation)\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "def preProcess(review):\n",
    "    token = word_tokenize(review) \n",
    "    tokens = [t.lower() for t in token]\n",
    "    filterd_words = [word for word in tokens if word not in stopWords]\n",
    "    return ' '.join(text for text in filterd_words if text not in punct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val = preProcess(\"This movie really sucks! Can I get my money back please?\")\n",
    "print(val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pre process step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'move run get'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imports\n",
    "import os\n",
    "from nltk import pos_tag\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "java_path = \"C:/Program Files/Java/jdk1.8.0_45/bin/java.exe\"\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "#os.environ['STANFORD_POSTAGGER'] = \"C:/Users/Me/Documents/stanford-postagger-full-2018-10-16\"\n",
    "    os.environ['STANFORD_POSTAGGER'] = \"D:/NLP/Library file/stanford-postagger-full-2018-10-16\"\n",
    "Stanford_postagger = StanfordPOSTagger(os.environ['STANFORD_POSTAGGER'] +'/models/english-bidirectional-distsim.tagger',os.environ['STANFORD_POSTAGGER']+'/stanford-postagger.jar')\n",
    "\n",
    "\n",
    "# You need to convert the tag from the pos_tagger to one of the four \"syntactic categories\" that wordnet recognizes,\n",
    "# then pass that to the lemmatizer as the word_pos.\n",
    "# so lemmatizer can process it\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None \n",
    "    \n",
    "#tagged is defined globally\n",
    "\n",
    "def nltk_lemma(tag):\n",
    "    global tagged\n",
    "    lst=[]\n",
    "    for word, tag in tagged:\n",
    "        \n",
    "        wntag = get_wordnet_pos(tag)\n",
    "        \n",
    "        if wntag is None:# not supply tag in case of None\n",
    "            lemma = nltk_lemmatizer.lemmatize(word)\n",
    "            lst.append(lemma)\n",
    "            #return \" \".join(lemma)\n",
    "            #print(lemma)\n",
    "        else:\n",
    "            lemma = nltk_lemmatizer.lemmatize(word, pos=wntag)\n",
    "            lst.append(lemma)\n",
    "            #return \" \".join(lemma)\n",
    "            #print(lemma)\n",
    "            \n",
    "    return lst\n",
    "\n",
    "\n",
    "def POS_lemmatize(review):\n",
    "    global tagged\n",
    "    #normal tagger\n",
    "    tagged = pos_tag(word_tokenize(review))\n",
    "    #stanford tagger\n",
    "    #tagged = Stanford_postagger.tag(word_tokenize(review))# imp word tokenize and repective POS\n",
    "    return \" \".join(nltk_lemma(tagged))# retuns setence after lemmatising\n",
    "\n",
    "\n",
    "POS_lemmatize(\"moving  running geting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# also 's\n",
    "def tokenize(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet)\n",
    "    tweet = re.sub(r\"#(\\w+)\", '', tweet)\n",
    "    tweet = re.sub(r\"@(\\w+)\", '', tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = tweet.strip().lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return tokens\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('D:\\dataset\\Sentiment analysis Data\\imdb_master.csv',encoding=\"latin\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>type</th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>Once again Mr. Costner has dragged out a movie...</td>\n",
       "      <td>neg</td>\n",
       "      <td>0_2.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>This is an example of why the majority of acti...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10000_4.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>test</td>\n",
       "      <td>First of all I hate those moronic rappers, who...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10001_1.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "      <td>Not even the Beatles could write songs everyon...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10002_3.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "      <td>Brass pictures (movies is not a fitting word f...</td>\n",
       "      <td>neg</td>\n",
       "      <td>10003_3.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  type                                             review label  \\\n",
       "0           0  test  Once again Mr. Costner has dragged out a movie...   neg   \n",
       "1           1  test  This is an example of why the majority of acti...   neg   \n",
       "2           2  test  First of all I hate those moronic rappers, who...   neg   \n",
       "3           3  test  Not even the Beatles could write songs everyon...   neg   \n",
       "4           4  test  Brass pictures (movies is not a fitting word f...   neg   \n",
       "\n",
       "          file  \n",
       "0      0_2.txt  \n",
       "1  10000_4.txt  \n",
       "2  10001_1.txt  \n",
       "3  10002_3.txt  \n",
       "4  10003_3.txt  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'type', 'review', 'label', 'file'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 5 columns):\n",
      "Unnamed: 0    100000 non-null int64\n",
      "type          100000 non-null object\n",
      "review        100000 non-null object\n",
      "label         100000 non-null object\n",
      "file          100000 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 3.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    12500\n",
       "type          12500\n",
       "review        12500\n",
       "label         12500\n",
       "file          12500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[df['type'] == 'test'].count()     #25000\n",
    "#df[df['type'] == 'train'].count()    #75000\n",
    "df[(df['label'] == 'pos') & (df['type'] == 'test')].count()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    12500\n",
       "type          12500\n",
       "review        12500\n",
       "label         12500\n",
       "file          12500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['label'] == 'pos') & (df['type'] == 'train')].count()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['type'] == 'train'].count()    #75000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unsup    50000\n",
       "neg      25000\n",
       "pos      25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
