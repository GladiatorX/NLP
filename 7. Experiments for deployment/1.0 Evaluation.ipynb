{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# validation split\n",
    "* Keras can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset each epoch.\n",
    "\n",
    " model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10)\n",
    "    \n",
    "* directly \n",
    "\n",
    "\n",
    "# Manual split\n",
    "*  train_test_split() function from the Python scikit-learn \n",
    "*  validation_data\n",
    "model.fit(X_train, y_train, validation_data=(X_test,y_test))\n",
    "\n",
    "# k fold\n",
    "* StratifiedKFold class from the scikit-learn Python machine learning library to split up the training dataset into k folds\n",
    "*  folds are stratified, meaning that the algorithm attempts to balance the number of instances of each class in each fold.\n",
    "\n",
    "* The verbose output for each epoch is turned off by passing verbose=0 to the fit() and evaluate() functions on the model.\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/evaluate-performance-deep-learning-models-keras/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other articel\n",
    "https://machinelearningmastery.com/use-keras-deep-learning-models-scikit-learn-python/\n",
    "\n",
    "QQ.\n",
    "Let’s assume I execute a k-fold just like you’ve done in your example. Then, I use the code from another of your great posts to save the model (to JSON) and save the weights. Which of the 10 models (created during the k-fold loop) will be saved? The last of the 10?\n",
    "I want to have a saved model that I can use on new datasets in the future. How does creating 10 models with k-fold help me get a better model than using an automatic validation split (as described in this post)?\n",
    "\n",
    "ANS.\n",
    "\n",
    "Jason Brownlee August 17, 2017 at 6:45 am # \n",
    "CV gives a less biased estimate of the models skill on unseen data than a train/test split, at least in general with smallish datasets (less than millions of obs).\n",
    "Once you have tuned your model, throw all the trained models away and train a final model with all your data and start using it to make predictions.\n",
    "See this post:\n",
    "http://machinelearningmastery.com/train-final-machine-learning-model/\n",
    "Some models are very expensive to train, in which case don’t use CV and keep the best models you train, use them in an ensemble as final models.\n",
    "\n",
    "\n",
    "https://machinelearningmastery.com/difference-test-validation-datasets/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# cross validation - model is doing training and testing directly\n",
    "\n",
    "Look I merged two of your examples: the one above and the Save and Load Your Keras Deep Learning Models (https://machinelearningmastery.com/save-load-keras-deep-learning-models/). You can see the code below:\n",
    "# MLP for Pima Indians Dataset with 10-fold cross validation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)\n",
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(“pima-indians-diabetes.csv”, delimiter=”,”)\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "#make model\n",
    "def make_model():\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation=’relu’))\n",
    "model.add(Dense(8, activation=’relu’))\n",
    "model.add(Dense(1, activation=’sigmoid’))\n",
    "# Compile model\n",
    "model.compile(loss=’binary_crossentropy’, optimizer=’adam’, metrics=[‘accuracy’])\n",
    "return(model)\n",
    "# define 10-fold cross validation test harness\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "for train, test in kfold.split(X, Y):\n",
    "# create model\n",
    "model = make_model()\n",
    "# Fit the model\n",
    "model.fit(X[train], Y[train], epochs=150, batch_size=10, verbose=0)\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(“model.json”, “w”) as json_file:\n",
    "json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(“model.h5”)\n",
    "print(“Saved model to disk”)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "print(“%s: %.2f%%” % (model.metrics_names[1], scores[1]*100))\n",
    "cvscores.append(scores[1] * 100)\n",
    "del model_json\n",
    "del model\n",
    "print(“%.2f%% (+/- %.2f%%)” % (numpy.mean(cvscores), numpy.std(cvscores)))\n",
    "# load json and create model\n",
    "json_file = open(‘model.json’, ‘r’)\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(“model.h5”)\n",
    "print(“Loaded model from disk”)\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss=’binary_crossentropy’, optimizer=’rmsprop’, metrics=[‘accuracy’])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(“%s: %.2f%%” % (loaded_model.metrics_names[1], score[1]*100))\n",
    "And the output is as follow:\n",
    "Saved model to disk\n",
    "acc: 76.62%\n",
    "Saved model to disk\n",
    "acc: 74.03%\n",
    "Saved model to disk\n",
    "acc: 71.43%\n",
    "Saved model to disk\n",
    "acc: 72.73%\n",
    "Saved model to disk\n",
    "acc: 70.13%\n",
    "Saved model to disk\n",
    "acc: 64.94%\n",
    "Saved model to disk\n",
    "acc: 66.23%\n",
    "Saved model to disk\n",
    "acc: 64.94%\n",
    "Saved model to disk\n",
    "acc: 63.16%\n",
    "Saved model to disk\n",
    "acc: 72.37%\n",
    "69.66% (+/- 4.32%)\n",
    "Loaded model from disk\n",
    "acc: 75.91%\n",
    "Naively I was expecting to get the save accuracy as the last model I saved (which was 72.37%), but I got 75.91%. Could you please explain how the weights are saved inside a k-fold cross validation?\n",
    "Thanks,\n",
    "Estelle\n",
    "Reply \n",
    "\n",
    "Jason Brownlee October 27, 2017 at 2:55 pm # \n",
    "Machine learning algorithms are stochastic, see this post for more details:\n",
    "https://machinelearningmastery.com/randomness-in-machine-learning/\n",
    "Reply \n",
    "\n",
    "Estelle October 28, 2017 at 7:26 am # \n",
    "Hi Jason,\n",
    "Thank you for your quick answer and reference.\n",
    "Yes, I forgot that randomness happens at so many levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV is for evaluating a model, then you can build a final model. \n",
    "* We want to know how skillful the model is on average, when trained on a random sample from the domain, and making predictions on other data in the domain.\n",
    "* To calculate this estimate, we use resampling methods like k-fold cross-validation that make good economical use of a fixed sized training dataset.\n",
    "* Once we select a model+config that has the best estimated skill, we can then train it on all of the available data and use it to start making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "You could plot learning curves for one train/test pass, or for all, for example:\n",
    "https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
